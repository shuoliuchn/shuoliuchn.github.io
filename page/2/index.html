<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="Tu5vqPaUb8svfkPx5eetJFD84ciQCcWVXNatdsWtj9Q">
  <meta name="baidu-site-verification" content="baidu_verify_FBq9PG4BBb">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"sliu.vip","root":"/","scheme":"Mist","version":"7.7.2","exturl":false,"sidebar":{"position":"right","width":240,"display":"hide","padding":12,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":false,"nav":null,"activeClass":"valine"},"algolia":{"appID":"PXY1Z6GHKT","apiKey":"eb82f2e78e3053f26aa408e9caa96d93","indexName":"blog","hits":{"per_page":10},"labels":{"input_placeholder":"要查点什么(✿◡‿◡)","hits_empty":"没有找到任何关于 ${query} 的结果╥﹏╥...","hits_stats":"搜索到 ${hits} 条记录，用时 ${time} ms o(*￣▽￣*)ブ"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="不成为自己讨厌的人">
<meta property="og:type" content="website">
<meta property="og:title" content="刘硕的技术查阅手册">
<meta property="og:url" content="https://sliu.vip/page/2/index.html">
<meta property="og:site_name" content="刘硕的技术查阅手册">
<meta property="og:description" content="不成为自己讨厌的人">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="刘硕">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="全栈开发">
<meta property="article:tag" content="并发编程">
<meta property="article:tag" content="网络编程">
<meta property="article:tag" content="爬虫">
<meta property="article:tag" content="数据库">
<meta property="article:tag" content="自动化测试">
<meta property="article:tag" content="Web开发">
<meta property="article:tag" content="HTML">
<meta property="article:tag" content="CSS">
<meta property="article:tag" content="JavaScript">
<meta property="article:tag" content="jQuery">
<meta property="article:tag" content="Bootstrap">
<meta property="article:tag" content="测试自动化">
<meta property="article:tag" content="Selenium">
<meta property="article:tag" content="postman">
<meta property="article:tag" content="Linux">
<meta property="article:tag" content="Vue">
<meta property="article:tag" content="drf">
<meta property="article:tag" content="django">
<meta property="article:tag" content="文档翻译">
<meta property="article:tag" content="PEP8">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://sliu.vip/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false
  };
</script>

  <title>刘硕的技术查阅手册</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?f14f123935d6183fdd06f8f1c4bc378f";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="刘硕的技术查阅手册" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">刘硕的技术查阅手册</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <h1 class="site-subtitle" itemprop="description">Python 全栈开发学习笔记</h1>
      
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-toc">

    <a href="/toc/" rel="section"><i class="fa fa-fw fa-book"></i>总目录</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/404/" rel="section"><i class="fa fa-fw fa-heartbeat"></i>公益 404</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container"></div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="algolia-results">
  <div id="algolia-stats"></div>
  <div id="algolia-hits"></div>
  <div id="algolia-pagination" class="algolia-pagination"></div>
</div>

  
</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sliu.vip/database/redis-abc/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="刘硕">
      <meta itemprop="description" content="不成为自己讨厌的人">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="刘硕的技术查阅手册">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/database/redis-abc/" class="post-title-link" itemprop="url">Redis 的基本使用</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-03-30 00:05:29 / 修改时间：00:05:32" itemprop="dateCreated datePublished" datetime="2020-03-30T00:05:29+08:00">2020-03-30</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/" itemprop="url" rel="index"><span itemprop="name">数据库</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/database/redis-abc/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/database/redis-abc/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="Redis-初识"><a href="#Redis-初识" class="headerlink" title="Redis 初识"></a>Redis 初识</h3><p>Redis 是一个高性能的 key-value 数据格式的内存缓存，是一种 NOSQL 数据库。</p>
<p>NOSQL，Not Only SQL，泛指非关系型数据库。</p>
<p>关系型数据库（MySQL、Oracle、SQL server、db2、SQLlite、PostgreSQL）的特征：</p>
<ol>
<li>数据存放在表中，表之间有关系</li>
<li>通用的 SQL 操作语言</li>
<li>大部分支持事务</li>
</ol>
<p>非关系型数据库（Redis、Hadoop、mangoDB）的特征：</p>
<ol>
<li>没有数据表的概念，不同的 NOSQL 数据库存放数据位置不同</li>
<li>NOSQL 数据库没有通用的操作语言</li>
<li>基本不支持事务，Redis 支持简单事务</li>
</ol>
<p>Redis 是一种内存型（数据存放在内存中）的非关系型（NOSQL）key-value（键值存储）数据库，支持数据的持久化（注：数据持久化时将数据存放到文件中，每次启动 Redis 之后会先将文件中数据加载到内存），经常用做缓存（用来缓存一些经常用到的数据，提高读写速度）。</p>
<p>Redis 是一款基于 C/S 架构的数据库，所以 Redis 既有客户端，也有服务端。</p>
<p>Redis 的客户端可以使用 Python 等编程语言，也可以终端命令行工具，还可以是 Redis Desktop Manager 管理工具：</p>
<img src="/database/redis-abc/1553246999266.png" class="" width="1553246999266">

<p>Redis 客户端连接服务器：</p>
<figure class="highlight x86asm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">redis-<span class="keyword">cli</span> -h redis服务器<span class="built_in">ip</span> -p redis服务器port</span><br></pre></td></tr></table></figure>

<h3 id="Redis-的几个站点地址"><a href="#Redis-的几个站点地址" class="headerlink" title="Redis 的几个站点地址"></a>Redis 的几个站点地址</h3><p>中文官网： <a href="http://www.redis.cn/" target="_blank" rel="noopener">http://www.redis.cn/</a></p>
<p>英文官网：<a href="https://redis.io" target="_blank" rel="noopener">https://redis.io</a></p>
<p>参考命令：<a href="http://doc.redisfans.com/" target="_blank" rel="noopener">http://doc.redisfans.com/</a></p>
<h3 id="Redis-的数据类型"><a href="#Redis-的数据类型" class="headerlink" title="Redis 的数据类型"></a>Redis 的数据类型</h3><p>Redis 中的数据类型主要有 5 中：string 字符串、hash 哈希、list 列表、set 无序集合以及 zset 有序集合。接下来，我们将逐一进行讨论。</p>
<h4 id="string-字符串类型"><a href="#string-字符串类型" class="headerlink" title="string 字符串类型"></a>string 字符串类型</h4><p>字符串类型是 Redis 中最为基础的数据存储类型，它在 Redis 中是二进制安全的，也就是字符串存储过程中，不会改变二进制数据的代码。比如 C 语言是二进制不安全的，因为 C 语言会在字符串末尾加上 <code>\0</code> 标识。Redis 是二进制安全的，就不会加上这种标识。</p>
<p>字符串类型的数据单个数据的最大容量是 512M。</p>
<figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">key</span>: <span class="keyword">string</span></span><br></pre></td></tr></table></figure>

<h4 id="hash-哈希类型"><a href="#hash-哈希类型" class="headerlink" title="hash 哈希类型"></a>hash 哈希类型</h4><p>hash 用于存储对象，对象的结构为属性、值，值的类型为 string。</p>
<figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">key: &#123;</span><br><span class="line">    域: 值[这里的值只能是字符串],</span><br><span class="line">    域: 值,</span><br><span class="line">    域: 值,</span><br><span class="line">    域: 值,</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="list-列表类型"><a href="#list-列表类型" class="headerlink" title="list 列表类型"></a>list 列表类型</h4><p>列表的元素类型为 string。</p>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">key: [ 值<span class="number">1</span>, 值<span class="number">2</span>, 值<span class="number">3</span> ...]</span><br></pre></td></tr></table></figure>

<h4 id="set-无序集合类型"><a href="#set-无序集合类型" class="headerlink" title="set 无序集合类型"></a>set 无序集合类型</h4><p>无序集合的元素为 string 类型，元素唯一不重复，没有修改操作。</p>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">key: &#123;值<span class="number">1</span>, 值<span class="number">4</span>, 值<span class="number">3</span>, 值<span class="number">5</span>&#125;</span><br></pre></td></tr></table></figure>

<h4 id="zset-有序集合类型"><a href="#zset-有序集合类型" class="headerlink" title="zset 有序集合类型"></a>zset 有序集合类型</h4><p>有序集合的官方名称是 sortedset，元素为 string 类型，元素唯一不重复，没有修改操作，按照权重值进行排序。</p>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">key: &#123;</span><br><span class="line">    值<span class="number">1</span> 权重值<span class="number">4</span>,</span><br><span class="line">    值<span class="number">2</span> 权重值<span class="number">3</span>,</span><br><span class="line">    值<span class="number">3</span> 权重值<span class="number">2</span>,</span><br><span class="line">    值<span class="number">4</span> 权重值<span class="number">1</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="通用命令"><a href="#通用命令" class="headerlink" title="通用命令"></a>通用命令</h3><p>启动 redis-cli 客户端连接服务端</p>
<p>如果在配置文件中设置了访问密码，则需要在连接 Redis 时填写密码。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">redis-cli -h &lt;IP地址，默认127.0.01&gt; -p &lt;端口,默认6379&gt;    <span class="comment"># 进入redis，如果是本地，则不需要声明-h参数。</span></span><br><span class="line">redis-cli auth &lt;密码&gt;    <span class="comment"># 填写密码</span></span><br></pre></td></tr></table></figure>

<p>切换数据库</p>
<p>默认情况下，我们在终端下面连接 Redis，进入的数据库是 0 号数据库。如果要切换数据库，则可以使用 select 命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select &lt;db&gt;</span><br></pre></td></tr></table></figure>

<h3 id="string-字符串操作"><a href="#string-字符串操作" class="headerlink" title="string 字符串操作"></a>string 字符串操作</h3><h4 id="增-改"><a href="#增-改" class="headerlink" title="增/改"></a>增/改</h4><p>设置键值，如果设置的键不存在则为添加，如果设置的键已经存在则修改</p>
<figure class="highlight q"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">set</span> <span class="built_in">key</span> <span class="built_in">value</span></span><br></pre></td></tr></table></figure>

<p>例：设置键为 name 值为 xiaoming 的数据:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set name xiaoming</span><br></pre></td></tr></table></figure>

<img src="/database/redis-abc/1553478355927.png" class="" width="1553478355927">

<p>设置键值及过期时间，以秒为单位</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">setex key seconds value</span><br></pre></td></tr></table></figure>

<p>例：设置键为 name 值为 xiaoming过期时间为 3 秒的数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">setex name 20 xiaoming</span><br></pre></td></tr></table></figure>

<p>查看有效时间，以秒为单位。如果没有设置过期时间，返回值为 -1，如果数据不存在，返回值为 -2。</p>
<figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">ttl key</span></span><br></pre></td></tr></table></figure>

<p>例：查看键 name 的有效时间</p>
<figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ttl <span class="built_in">name</span></span><br></pre></td></tr></table></figure>

<p>string 是 Redis 中唯一能够设置保存数据有效期的数据类型。我们可以通过两种方式设定数据的有效期：</p>
<ul>
<li><p>setex 添加保存数据到 redis，同时设置有效期，格式为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">setex key time value</span><br></pre></td></tr></table></figure>
</li>
<li><p>expire 给已有的数据重新设置有效期，格式为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">expire key time</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>批量设置多个键值。如果有很多数据，如果每次设置一组键值对，那真的是很蛮烦。这时，我们可以使用 mset，一次性设置多个键值：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mset key1 value1 key2 value2 ...</span><br></pre></td></tr></table></figure>

<p>例：设置键为 a1 值为 python、键为 a2 值为 java、键为 a3 值为 c</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mset a1 python a2 java a3 c</span><br></pre></td></tr></table></figure>

<p>拼接值</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">append key value</span><br></pre></td></tr></table></figure>

<p>例：向键为 title 的值拼接 llo</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">set title he</span><br><span class="line">append title llo </span><br><span class="line">get title</span><br></pre></td></tr></table></figure>

<p>输出结果：<code>hello</code></p>
<h4 id="查"><a href="#查" class="headerlink" title="查"></a>查</h4><p>根据键获取值，如果不存在此键则返回 <code>(nil)</code>，一次只能查看一个数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">get key</span><br></pre></td></tr></table></figure>

<p>例5：获取键 name 的值</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">get name</span><br></pre></td></tr></table></figure>

<p>根据多个键获取多个值</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mget key1 key2 ...</span><br></pre></td></tr></table></figure>

<p>例：获取键 a1、a2、a3 的值</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mget a1 a2 a3</span><br></pre></td></tr></table></figure>

<h4 id="删"><a href="#删" class="headerlink" title="删"></a>删</h4><p>删除指定键对应的值，可以同时删除多个：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">del key1 key2 key3 ...</span><br></pre></td></tr></table></figure>

<p>例：删除 title：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">del title</span><br></pre></td></tr></table></figure>

<h3 id="键操作"><a href="#键操作" class="headerlink" title="键操作"></a>键操作</h3><h4 id="查-1"><a href="#查-1" class="headerlink" title="查"></a>查</h4><p>查找当前数据库中的键，参数中可用 <code>*</code> 指代多个或 0 个任意字符，使用 <code>?</code> 指代 1 个任意字符。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keys pattern</span><br></pre></td></tr></table></figure>

<p>例：查看所有键</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keys *</span><br></pre></td></tr></table></figure>

<p>例：查看名称中包含 a 的键</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keys a*</span><br></pre></td></tr></table></figure>

<img src="/database/redis-abc/1585410303920.png" class="" width="1585410303920">

<p>判断键是否存在，如果存在返回 1，不存在返回 0。如果是多个数据，返回的是存在的个数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">exists key</span><br><span class="line">exists key1 key2 key3 ...</span><br></pre></td></tr></table></figure>

<p>例：判断键 a1 是否存在</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">exists a1</span><br></pre></td></tr></table></figure>

<img src="/database/redis-abc/1585409422569.png" class="" width="1585409422569">

<p>查看键对应的 value 的类型，返回的值为 redis 支持的五种类型中的⼀种</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">type key</span><br></pre></td></tr></table></figure>

<p>例：查看键 a1 的值类型</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">type a1</span><br></pre></td></tr></table></figure>

<h4 id="删-1"><a href="#删-1" class="headerlink" title="删"></a>删</h4><p>删除键及对应的值</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">del key1 key2 ...</span><br></pre></td></tr></table></figure>

<p>例5：删除键 a2 和 a3</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">del a2 a3</span><br></pre></td></tr></table></figure>

<p>清空当前数据库中所有的键</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flushall</span><br></pre></td></tr></table></figure>

<h3 id="hash-哈希操作"><a href="#hash-哈希操作" class="headerlink" title="hash 哈希操作"></a>hash 哈希操作</h3><p>哈希表，类似于 Python 的字典，其基本建构为：</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">键<span class="string">key:</span> &#123;</span><br><span class="line">   	域<span class="string">field:</span> 值value,</span><br><span class="line">   	域<span class="string">field:</span> 值value,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="增-改-1"><a href="#增-改-1" class="headerlink" title="增/改"></a>增/改</h4><p>设置单个属性</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hset key field value</span><br></pre></td></tr></table></figure>

<p>例：设置键 user 的属性 name 为 xiaohong</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hset user name xiaohong</span><br></pre></td></tr></table></figure>

<p>批量设置多个属性</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hmset key field1 value1 field2 value2 ...</span><br></pre></td></tr></table></figure>

<p>例：设置键 user2 的属性 name 为 xiaohong、属性 age 为 11</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hmset user2 name xiaohongage age 11</span><br></pre></td></tr></table></figure>

<h4 id="查-2"><a href="#查-2" class="headerlink" title="查"></a>查</h4><p>获取指定键所有的属性</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hkeys key</span><br></pre></td></tr></table></figure>

<p>例：获取键 user2 的所有属性</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hkeys user2</span><br></pre></td></tr></table></figure>

<p>获取⼀个属性的值</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hget key field</span><br></pre></td></tr></table></figure>

<p>例：获取键 user2 属性 name 的值</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hget user2 name</span><br></pre></td></tr></table></figure>

<p>获取多个属性的值</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hmget key field1 field2 ...</span><br></pre></td></tr></table></figure>

<p>例：获取键 user2 属性 name、age 的值</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hmget user2 name age</span><br></pre></td></tr></table></figure>

<p>获取所有属性的值</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hvals key</span><br></pre></td></tr></table></figure>

<p>例：获取键 user2 所有属性的值</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hvals user2</span><br></pre></td></tr></table></figure>

<p>获取成员个数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hlen key</span><br></pre></td></tr></table></figure>

<p>例：获取jian user2 的成员个数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hlen user2</span><br></pre></td></tr></table></figure>

<p>输出结果：3</p>
<h4 id="删-2"><a href="#删-2" class="headerlink" title="删"></a>删</h4><p>删除属性，属性对应的值会被⼀起删除</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdel key field1 field2 ...</span><br></pre></td></tr></table></figure>

<p>例：删除键 u2 的属性 age</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdel u2 age</span><br></pre></td></tr></table></figure>

<h3 id="list-列表操作"><a href="#list-列表操作" class="headerlink" title="list 列表操作"></a>list 列表操作</h3><p>列表中的元素类型只能为 string，列表元素按照插入顺序排序。</p>
<h4 id="增"><a href="#增" class="headerlink" title="增"></a>增</h4><p>在列表左侧插⼊数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lpush key value1 value2 ...</span><br></pre></td></tr></table></figure>

<p>例：向键为 a1 的列表左侧加⼊数据 a、b、c</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lpush a1 a b c</span><br></pre></td></tr></table></figure>

<p>在列表右侧插⼊数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpush key value1 value2 ...</span><br></pre></td></tr></table></figure>

<p>例：从键为 a1 的列表右侧插入数据 0、1</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpush a1 0 1</span><br></pre></td></tr></table></figure>

<p>在指定元素的前或后插⼊新元素</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">linsert key before&#x2F;after 现有元素 新元素</span><br></pre></td></tr></table></figure>

<p>例：在键为 a1 的列表中元素 b 前插入 3</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">linsert a1 before b 3</span><br></pre></td></tr></table></figure>

<h4 id="改"><a href="#改" class="headerlink" title="改"></a>改</h4><p>设置指定索引位置的元素值</p>
<ul>
<li><p>索引从左侧开始，第⼀个元素为0</p>
</li>
<li><p>索引可以是负数，表示尾部开始计数，如<code>-1</code>表示最后⼀个元素</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lset key index value</span><br></pre></td></tr></table></figure>

<p>例：修改键为 a1 的列表中下标为 1 的元素值为 z</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lset a1 1 z</span><br></pre></td></tr></table></figure>

<p>对列表进行裁剪切片操作，切片索引超出范围不报错：</p>
<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ltrim key <span class="literal">start</span> <span class="literal">stop</span></span><br></pre></td></tr></table></figure>

<p>例：裁剪 a1 列表，只保存第 2 到第 4 个元素</p>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ltrim a1 <span class="number">2</span> <span class="number">4</span></span><br></pre></td></tr></table></figure>

<h4 id="查-3"><a href="#查-3" class="headerlink" title="查"></a>查</h4><p>获取指定列表的长度</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">llen key</span><br></pre></td></tr></table></figure>

<p>例：查看列表 a1 的长度</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">llen a1</span><br></pre></td></tr></table></figure>

<p>查看一定索引范围内的列表元素，起始和终止索引对应的元素都会被查询到（顾头顾腚）</p>
<figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">lrange</span> key start stop</span><br></pre></td></tr></table></figure>

<p>例：查看 a1 列表第 2 到第 5 个元素；查看 a1 列表的全部元素</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lrange a1 2 5</span><br><span class="line">lrange a1 0 -1</span><br></pre></td></tr></table></figure>

<p>通过索引获取列表中的元素</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lindex key index</span><br></pre></td></tr></table></figure>

<p>例：查看 a1 列表中，索引为 1 的数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lindex a1 1</span><br></pre></td></tr></table></figure>

<h4 id="删-3"><a href="#删-3" class="headerlink" title="删"></a>删</h4><p>删除指定元素</p>
<p>将列表中前 count 次出现的值为 value 的元素移除</p>
<ul>
<li><code>count &gt; 0</code>：从头往尾移除，如果 <code>count == 2</code>，则从头开始，删除 2 个成员 value</li>
<li><code>count &lt; 0</code>：从尾往头移除，如果 <code>count == -2</code>，则从尾开始，删除 2 个成员 value</li>
<li><code>count == 0</code>：移除所有</li>
</ul>
<figure class="highlight q"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lrem <span class="built_in">key</span> <span class="built_in">count</span> <span class="built_in">value</span></span><br></pre></td></tr></table></figure>

<img src="/database/redis-abc/1553479815621.png" class="" width="1553479815621">

<p>例：创建列表 a2，向其中插入元素 <code>a b c a b c a b c</code>。然后从 a2 列表右侧开始删除 2 个 b，从左侧开始删除 1 个 a，删除所有 c</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lpush a2 a b c a b c a b c</span><br><span class="line">lrem a2 -2 b</span><br><span class="line">lrem a2 1 a</span><br><span class="line">lrem a2 0 c</span><br></pre></td></tr></table></figure>

<p>注意，上面的例子中，因为是向左侧插入，所以生成的列表其实是 <code>c b a c b a c b a</code>。</p>
<p>移出并获取列表左侧第一个元素</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lpop key</span><br></pre></td></tr></table></figure>

<p>例：移除列表 a2 左侧第一个元素并返回结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lpop a2</span><br></pre></td></tr></table></figure>

<p>移除列表右侧第一个元素，并将移除的元素返回。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpop key</span><br></pre></td></tr></table></figure>

<p>例：移除列表 a2 右侧第一个元素</p>
<figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">rpop</span> <span class="built_in">a2</span></span><br></pre></td></tr></table></figure>

<h3 id="set-无序集合操作"><a href="#set-无序集合操作" class="headerlink" title="set 无序集合操作"></a>set 无序集合操作</h3><h4 id="增-1"><a href="#增-1" class="headerlink" title="增"></a>增</h4><p>添加元素。无序集合是去重复的，若元素不存在会增加，若元素存在则不增加</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sadd key member1 member2 ...</span><br></pre></td></tr></table></figure>

<p>例：向键 a3 的集合中添加元素 zhangsan、lisi、wangwu</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sadd a3 zhangsan sili wangwu</span><br></pre></td></tr></table></figure>

<h4 id="查-4"><a href="#查-4" class="headerlink" title="查"></a>查</h4><p>查看集合中所有的元素</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">smembers key</span><br></pre></td></tr></table></figure>

<p>例：获取键 a3 的集合中所有元素</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">smembers a3</span><br></pre></td></tr></table></figure>

<p>获取集合的成员个数</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">sadd</span> <span class="string">name_list liubei caocao sunquan</span></span><br><span class="line"><span class="attr">scard</span> <span class="string">name_list</span></span><br></pre></td></tr></table></figure>

<p>输出结果：3</p>
<p>随机抽取任意数目（默认为一个）集合的成员，不会删除这些成员</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">srandmember key [count]</span><br></pre></td></tr></table></figure>

<p>例：随机抽取一个 a3 集合的成员，随机抽取两个 name_list 集合的成员</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">srandmember a3</span><br><span class="line">srandmember name_list</span><br></pre></td></tr></table></figure>

<h4 id="删-4"><a href="#删-4" class="headerlink" title="删"></a>删</h4><p>删除指定元素</p>
<figure class="highlight q"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">srem <span class="built_in">key</span> <span class="built_in">value</span></span><br></pre></td></tr></table></figure>

<p>例：删除键 a3 集合中的元素 wangwu</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">srem a3 wangwu</span><br></pre></td></tr></table></figure>

<p>随机抽取指定个数（默认为一个）成员返回并从集合中删除这个成员</p>
<figure class="highlight q"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spop <span class="built_in">key</span> [<span class="built_in">count</span>]</span><br></pre></td></tr></table></figure>

<p>例：随机抽取并删除集合 a3 中的一个成员，随机抽取并删除集合 name_list 中的两个成员</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spop a3</span><br><span class="line">spop name_list 2</span><br></pre></td></tr></table></figure>

<h3 id="zset-有序集合操作（了解）"><a href="#zset-有序集合操作（了解）" class="headerlink" title="zset 有序集合操作（了解）"></a>zset 有序集合操作（了解）</h3><p>Redis 的有序集合并不常用，这里简单介绍几个用法。</p>
<h4 id="增-2"><a href="#增-2" class="headerlink" title="增"></a>增</h4><p>向有序集合中增加一个或多个元素</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zadd key score1 member1 score2 member2</span><br></pre></td></tr></table></figure>

<p>例：向有序集合中插入数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zadd brother 1 liubei 2 guanyu 3 zhangfei</span><br></pre></td></tr></table></figure>

<h4 id="查-5"><a href="#查-5" class="headerlink" title="查"></a>查</h4><p>查询一定权重范围内的有序集合数据。加上 withscores，可以把权重打印出来</p>
<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zrange key <span class="literal">start</span> <span class="literal">stop</span> [withscores]</span><br></pre></td></tr></table></figure>

<p>例：查看有序集合 brother 的第 2 到第 3 个元素，并显示每个元素的权重</p>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zrange brother <span class="number">1</span> <span class="number">2</span> withscores</span><br></pre></td></tr></table></figure>

<p>查询有序列表中，元素的总个数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zcard key</span><br></pre></td></tr></table></figure>

<h4 id="删-5"><a href="#删-5" class="headerlink" title="删"></a>删</h4><p>删除有序集合中指定的元素，可同时删除多个：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zrem key member1 member2 ...</span><br></pre></td></tr></table></figure>

<p>例：删除有序集合 brother 的元素 guanyu 和 zhangfei</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zrem brother guanyu zhangfei</span><br></pre></td></tr></table></figure>

<h3 id="Redis-中各个数据类型的应用场景"><a href="#Redis-中各个数据类型的应用场景" class="headerlink" title="Redis 中各个数据类型的应用场景"></a>Redis 中各个数据类型的应用场景</h3><p>针对各种数据类型它们的特性，使用场景如下:</p>
<ul>
<li><p>字符串 string：用于保存一些项目中的普通数据或者<strong>有时效</strong>的数据，只要键值对的都可以保存，例如，保存 session。定时记录状态，保存一个倒计时时间值。</p>
</li>
<li><p>哈希 hash：用于保存项目中的一些字典数据，但是不能保存多维的字典。例如，商城的购物车或者登陆用户的信息。</p>
</li>
<li><p>列表 list：用于保存项目中的列表数据，但是也不能保存多维的列表。例如，队列，秒杀系统，挂号系统，排单系统，访问历史记录。</p>
</li>
<li><p>无序集合 set：用于保存项目中的一些不能重复的数据，可以用于过滤。例如，投票海选的时候，过滤候选人，收藏（去重）。</p>
</li>
<li><p>有序集合 zset：用于保存项目中一些不能重复，但是需要进行排序的数据。例如，分数排行榜。</p>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sliu.vip/database/redis-install/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="刘硕">
      <meta itemprop="description" content="不成为自己讨厌的人">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="刘硕的技术查阅手册">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/database/redis-install/" class="post-title-link" itemprop="url">Redis 的安装和配置</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-03-30 00:05:29 / 修改时间：00:05:32" itemprop="dateCreated datePublished" datetime="2020-03-30T00:05:29+08:00">2020-03-30</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/" itemprop="url" rel="index"><span itemprop="name">数据库</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/database/redis-install/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/database/redis-install/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Redis 是一款高性能，内存数据存储的非关系型数据库。通常用来进行数据缓存，也就是存放一些需要经常读取的数据。</p>
<p>Redis 官方原版网址：<a href="https://redis.io/" target="_blank" rel="noopener">https://redis.io/</a></p>
<p>Redis 中文官网：<a href="http://www.redis.cn" target="_blank" rel="noopener">http://www.redis.cn</a></p>
<h3 id="Windows-系统-Redis-的下载和安装"><a href="#Windows-系统-Redis-的下载和安装" class="headerlink" title="Windows 系统 Redis 的下载和安装"></a>Windows 系统 Redis 的下载和安装</h3><p>Redis 官方已经不再提供 Windows 版本的 Redis。我们现在常用的 Windows 系统的 Redis 数据库是微软团队根据官方的 Linux 版本高仿的。正因如此，我们不太会在生产环境中，使用 Windows 的 Redis。但是如果在学习测试的情况下，Windows 系统中的 Redis 已经足够了。</p>
<p>下载地址：<a href="https://github.com/MicrosoftArchive/redis/releases" target="_blank" rel="noopener">https://github.com/MicrosoftArchive/redis/releases</a></p>
<img src="/database/redis-install/1585392241391.png" class="" width="1585392241391">

<p>下载好之后，双击 msi 安装包，基本上一路点击下一步即可。只有两个地方需要稍微注意，不注意的话问题也不大。</p>
<p>首先是这里，安装路径当然不要有中文、空格和特殊符号。最好勾选上把安装路径加入到环境变量中。</p>
<img src="/database/redis-install/867021-20190120213613576-1092651557.png" class="" title="867021-20190120213613576-1092651557">

<p>然后是这里，默认端口一般就是 6379 不要变。像这种官方默认的端口号，如果不是有不可避免的冲突，不要轻易改。MySQL 默认就是 3306，Redis 就是 6379。这样以后配置起来都使用默认的，不至于乱套。</p>
<p>下面的让防火墙忽略也要勾选上。</p>
<img src="/database/redis-install/867021-20190120213732137-1070050780.png" class="" title="867021-20190120213732137-1070050780">

<p>这个无所谓，看需求咯。</p>
<img src="/database/redis-install/867021-20190120213836094-663215847.png" class="" title="867021-20190120213836094-663215847">

<p>然后就没啥好说的，一路下一步就好。</p>
<h3 id="启动和关闭-Redis-服务"><a href="#启动和关闭-Redis-服务" class="headerlink" title="启动和关闭 Redis 服务"></a>启动和关闭 Redis 服务</h3><p>使用以下命令启动 Redis 服务端，<code>redis.windows.conf</code> 文件就在刚刚安装的时候指定到 Redis 的安装路径中：</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">redis-server <span class="string">C:</span><span class="regexp">/redis/</span>redis.windows.conf</span><br></pre></td></tr></table></figure>

<img src="/database/redis-install/1585392867082.png" class="" width="1585392867082">

<p>这样开启 Redis 服务有个弊端是，关闭上面这个 cmd 窗口就关闭 Redis 服务器服务了。</p>
<p>若要 Redis 长期在后台运行，不会随着我们关闭 cmd 窗口而关闭，我们可以让 Redis 作为 Windows 服务的方式启动（同样，注意 <code>redis.windows.conf</code> 的位置在 Redis 的安装路径中）：</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">redis-server --service-install <span class="string">C:</span><span class="regexp">/redis/</span>redis.windows.conf</span><br></pre></td></tr></table></figure>

<p>若按下回车后一段时间无响应，可以使用 ctrl + C 强制退出。这没关系，你会看到，服务成功安装了：</p>
<img src="/database/redis-install/1585396298056.png" class="" width="1585396298056">

<p>客户端启动和终止 Redis 服务：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">redis-server --service-start    <span class="comment"># 启动Redis服务</span></span><br><span class="line">redis-server --service-stop    <span class="comment"># 终止Redis服务</span></span><br></pre></td></tr></table></figure>

<p>启动内置客户端连接 Redis 服务：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">redis-cli</span><br></pre></td></tr></table></figure>

<img src="/database/redis-install/1585396527990.png" class="" width="1585396527990">

<h3 id="Ubuntu-系统-Redis-的安装"><a href="#Ubuntu-系统-Redis-的安装" class="headerlink" title="Ubuntu 系统 Redis 的安装"></a>Ubuntu 系统 Redis 的安装</h3><p>Ubuntu 下安装 Redis 就十分简单了，一条命令即可：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install redis-server</span><br></pre></td></tr></table></figure>

<p>启动 Redis 服务同样只需一条命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">redis-server</span><br></pre></td></tr></table></figure>

<p>启动内置的客户端连接 Redis 服务：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">redis-cli</span><br></pre></td></tr></table></figure>

<h3 id="Redis-的配置"><a href="#Redis-的配置" class="headerlink" title="Redis 的配置"></a>Redis 的配置</h3><p>Redis 安装成功以后，Window 下的配置文件保存在 Redis 的安装目录下，文件名为 <code>redis.windows.conf</code>。如果是 macOS 或者 Linux，配置文件则位于 <code>/etc/redis/redis.conf</code></p>
<p>绑定 IP，如果需要远程访问，可将此行注释，或绑定⼀个真实 IP：</p>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bind <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>  ::<span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>端⼝，默认为 6379</p>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">port <span class="number">6379</span></span><br></pre></td></tr></table></figure>

<p>是否以守护进程运行（这里的配置主要是 Linux 和 macOS 下面需要配置的）</p>
<ul>
<li>如果以守护进程运行，则不会在命令行阻塞，类似于服务</li>
<li>如果以非守护进程运行，则在启动服务时，当前终端将会被阻塞</li>
<li>设置为 yes 表示守护进程，设置为 no 表示非守护进程</li>
<li>推荐设置为 yes</li>
</ul>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">daemonize</span> <span class="literal">yes</span></span><br></pre></td></tr></table></figure>

<p>数据文件</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">dbfilename</span> <span class="selector-tag">dump</span><span class="selector-class">.rdb</span></span><br></pre></td></tr></table></figure>

<p>数据文件存储路径</p>
<figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dir /var/<span class="class"><span class="keyword">lib</span>/<span class="title">redis</span></span></span><br></pre></td></tr></table></figure>

<p>日志文件保存位置</p>
<figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">logfile /<span class="built_in">var</span>/<span class="built_in">log</span>/redis/redis-server.<span class="built_in">log</span></span><br></pre></td></tr></table></figure>

<p>数据库数目，默认有 16 个</p>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">database <span class="number">16</span></span><br></pre></td></tr></table></figure>

<p>主从复制，类似于双机备份</p>
<figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">slaveof</span></span><br></pre></td></tr></table></figure>

<p>访问密码，连接 Redis 时需要输入指定密码，默认是注释状态的，一般情况也不会开启密码，因为 Redis 多数情况都是运行在 127.0.0.1，不会被外网访问到</p>
<figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">requirepass foobared</span></span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sliu.vip/database/redis-python/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="刘硕">
      <meta itemprop="description" content="不成为自己讨厌的人">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="刘硕的技术查阅手册">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/database/redis-python/" class="post-title-link" itemprop="url">Python 操作 Redis</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-03-30 00:05:29 / 修改时间：00:05:32" itemprop="dateCreated datePublished" datetime="2020-03-30T00:05:29+08:00">2020-03-30</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/" itemprop="url" rel="index"><span itemprop="name">数据库</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/database/redis-python/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/database/redis-python/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Python 有很多的模块都可以实现对 Redis 的操作，常用有 redis 和 pyredis，这两个模块的使用操作是类似的。</p>
<p>这里我们使用 redis 模块来进行演示。</p>
<p>安装 redis 模块：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install redis</span><br></pre></td></tr></table></figure>

<p>代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> redis <span class="keyword">import</span> Redis</span><br><span class="line"><span class="comment"># redis链接</span></span><br><span class="line"><span class="comment"># redis = Redis(host="127.0.0.1",port=6379,db=1)</span></span><br><span class="line"><span class="comment"># 如果设置了密码</span></span><br><span class="line">redis = Redis(host=<span class="string">"127.0.0.1"</span>,port=<span class="number">6379</span>,db=<span class="number">1</span>,password=<span class="string">"123456"</span>)</span><br><span class="line"><span class="string">"""字符串操作"""</span></span><br><span class="line"><span class="comment"># 添加一个字符串数据 set name xiaoming</span></span><br><span class="line"><span class="comment"># redis.set("name","xiaoming")</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加一个临时数据, setex title 30 hello</span></span><br><span class="line"><span class="comment"># redis.setex("title",30,"hello")</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看一个数据的有效期,-2表示过期,-1表示永久</span></span><br><span class="line"><span class="comment"># time = redis.ttl("title")</span></span><br><span class="line"><span class="comment"># print(time)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取一个字符串</span></span><br><span class="line"><span class="comment"># name = redis.get("name")</span></span><br><span class="line"><span class="comment"># print(name)  # 得到的结果是bytes类型的   b'xiaoming'</span></span><br><span class="line"><span class="comment"># print(name.decode()) # 必须要解码,xiaoming</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除key,因为del是一个关键词,所以在redis模块,凡是命令如果是一个关键词,全部改成单词的全拼</span></span><br><span class="line"><span class="comment"># redis.delete("name")</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 哈希的操作</span></span><br><span class="line"><span class="comment"># dict1 = &#123;</span></span><br><span class="line"><span class="comment">#     "liubei": 28,</span></span><br><span class="line"><span class="comment">#     "guanyu": 20,</span></span><br><span class="line"><span class="comment">#     "zhangfei": 14,</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br><span class="line"><span class="comment"># redis.hmset("brother",dict1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取哈希里面的所有成员</span></span><br><span class="line">dict_data = redis.hgetall(<span class="string">"brother"</span>)</span><br><span class="line">print(dict_data) <span class="comment"># &#123;b'liubei': b'28', b'guanyu': b'20', b'zhangfei': b'14'&#125;</span></span><br><span class="line"><span class="keyword">for</span> key,name <span class="keyword">in</span> dict_data.items():</span><br><span class="line">    print(key.decode(),name.decode())</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    liubei 28</span></span><br><span class="line"><span class="string">    guanyu 20</span></span><br><span class="line"><span class="string">    zhangfei 14</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">age = dict_data.get(<span class="string">"liubei"</span>.encode()).decode()</span><br><span class="line">print(age) <span class="comment"># b'28'</span></span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sliu.vip/notes/grokking-algorithms-bfs/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="刘硕">
      <meta itemprop="description" content="不成为自己讨厌的人">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="刘硕的技术查阅手册">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/notes/grokking-algorithms-bfs/" class="post-title-link" itemprop="url">广度优先搜索</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-03-30 00:05:29" itemprop="dateCreated datePublished" datetime="2020-03-30T00:05:29+08:00">2020-03-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-04-01 00:56:19" itemprop="dateModified" datetime="2020-04-01T00:56:19+08:00">2020-04-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">学习实践笔记</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/notes/grokking-algorithms-bfs/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/notes/grokking-algorithms-bfs/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>广度优先搜索让我们能够找出两样东西之间的最短距离。不过最短距离的含义有很多，使用广度优先搜索可以：</p>
<ul>
<li>编写国际跳棋 AI，计算最少走多少步就可获胜</li>
<li>编写拼写检査器，计算最少编辑多少个地方就可将错拼的单词改成正确的单词，如将 READED 改为 READER 只编辑一个字母改如何操作</li>
<li>根据你的人际关系网络找到关系最近的医生。</li>
</ul>
<h3 id="图简介"><a href="#图简介" class="headerlink" title="图简介"></a>图简介</h3><p>假设你居住在旧金山，要从双子峰前往金门大桥。你想乘公交车前往，并希望换乘最少。可乘坐的公交车如下：</p>
<img src="/notes/grokking-algorithms-bfs/1585495772598.png" class="" width="1585495772598">

<p>显然，我们至少需要三步才能从双子峰走到金门大桥：</p>
<img src="/notes/grokking-algorithms-bfs/1585495818790.png" class="" width="1585495818790">

<p>当然还有其他前往金门大桥的路线，但它们更远（需要四步）。这种问题被称为最短路径问题（shortest-path problem）。</p>
<p>我们经常要找出最短路径，这可能是前往朋友家的最短路径，也可能是国际象棋中把对方将死的最少步数。解决最短路径问题的算法被称为广度优先搜索。</p>
<p>要确定如何从双子峰前往金门大桥，需要两个步骤：</p>
<ol>
<li>使用图来建立问题模型。</li>
<li>使用广度优先搜索解决问题。</li>
</ol>
<p>图模拟一组连接。例如，假设你与朋友玩牌，并要模拟谁欠谁钱，可像下面这样指出 Alex 欠 Rama 钱：</p>
<img src="/notes/grokking-algorithms-bfs/1585495990061.png" class="" width="1585495990061">

<p>完整的欠钱关系图可能是这样的：</p>
<img src="/notes/grokking-algorithms-bfs/1585496028861.png" class="" width="1585496028861">

<p>Alex欠 Rama 钱，Tom 欠 Adit 钱，等等。图由节点（node）和边（edge）组成：</p>
<img src="/notes/grokking-algorithms-bfs/1585496094162.png" class="" width="1585496094162">

<p>一个节点可能与众多节点直接相连，这些节点被称为邻居。在前面的欠钱图中，Rama 是 Alex 的邻居。Adit 不是 Alex 的邻居，因为他们不直接相连。但 Adit 既是 Rama 的邻居，又是 Tom 的邻居。</p>
<p>图用于模拟不同的东西是如何相连的。</p>
<p>#<br>广度优先搜索是一种用于图的查找算法，可帮助回答两类问题：</p>
<ol>
<li>从节点 A 出发，有前往节点 B 的路径吗？</li>
<li>从节点 A 出发，前往节点 B 的哪条路径最短？</li>
</ol>
<p>前面计算从双子峰前往金门大桥的最短路径的问题，就属于第二类问题：哪条路径最短？</p>
<p>假设你经营着一个芒果农场，需要寻找芒果销售商，以便将芒果卖给他。在 Facebook，你与芒果销售商有联系吗？为此，你可以在朋友中查找。</p>
<p>首先，创建一个朋友的清单，然后依次检查每个朋友，看他是否是芒果经销商。如果是，则大功告成，否则查看下一个朋友。</p>
<p>如果你的朋友没有任何一个人是芒果经销商，你就需要在朋友的朋友中寻找：</p>
<img src="/notes/grokking-algorithms-bfs/1585496587854.png" class="" width="1585496587854">

<p>这次，你还是从自己的朋友开始找起。不同的是，如果你的朋友不是经销商，你就把朋友的人际关系，也就是朋友的所有朋友也加入到查找清单中。</p>
<img src="/notes/grokking-algorithms-bfs/1585496660449.png" class="" width="1585496660449">

<p>这样一来，你不仅在朋友中查找，还在朋友的朋友中查找。别忘了，你的目标是在人际关系网中找到一位芒果销售商。因此，如果 Alice 不是芒果销售商，就将其朋友也加入到名单中。这意味着你将在她的朋友、朋友的朋友等中査找。使用这种算法将搜遍你的整个人际关系网，直到找到芒果销售商。这就是广度优先搜索算法</p>
<h3 id="查找最短路径"><a href="#查找最短路径" class="headerlink" title="查找最短路径"></a>查找最短路径</h3><p>再说一次，广度优先搜索可回答两类问题：</p>
<ol>
<li><p>从节点 A 出发，有前往节点 B 的路径吗？（在你的人际关系网中，有芒果销售商吗？）</p>
</li>
<li><p>从节点A出发，前往节点B的哪条路径最短？（哪个芒果销售商与你的关系最近？）</p>
</li>
</ol>
<p>刚刚我们讨论了如何回答第一类问题，下面来尝试回答第二类问题——谁是关系最近的芒果销售商。例如，朋友是一度关系，朋友的朋友是二度关系</p>
<p>显而易见，一度关系胜过二度关系，二度关系胜过三度关系，以此类推。因此，你应先在一度关系中搜索，确定其中没有芒果销售商后，才在二度关系中搜索。</p>
<p>广度优先搜索就是这样做的！在广度优先搜索的执行过程中，搜索范围从起点开始逐渐向外延伸。即先检查一度关系，再检查二度关系。</p>
<p>Claire 是一度关系，而 Anuj 是二度关系，因此将先检查 Claire，后检查 Anuj。你也可以这样看，一度关系在二度关系之前加入查找名单。你按顺序依次检査名单中的每个人，看看他是否是芒果销售商。这将先在一度关系中查找，再在二度关系中查找，因此找到的是关系最近的芒果销售商。广度优先搜索不仅査找从 A 到 B 的路径，而且找到的是最短的路径。</p>
<img src="/notes/grokking-algorithms-bfs/1585544498067.png" class="" width="1585544498067">

<p>注意，只有按添加顺序査找时，才能实现这样的目的。换句话说，如果 Claire 先于 Anuj 加入名单，就需要先检查 Claire，再检查 Anuj。如果 Claire 和 Anuj 都是芒果销售商，而你先检查 Anuj 再检查 Claire，结果将如何呢？找到的芒果销售商并非是与你关系最近的，因为 Anuj 是你朋友的朋友，而 Claire是你的朋友。因此，你需要按添加顺序进检查。</p>
<p>这种先添加先检查的数据结构，就是队列。</p>
<h3 id="队列"><a href="#队列" class="headerlink" title="队列"></a>队列</h3><p>队列类似于栈，你不能随机地访问队列中的元素。队列只支持两种操作：入队和出队。</p>
<img src="/notes/grokking-algorithms-bfs/1585582209888.png" class="" width="1585582209888">

<p>如果你将两个元素加入队列，先加入的元素将在后加入的元素之前出队。因此，你可使用队列来表示査找名单，这样，先加入的人将先出队并先被检查。</p>
<p>队列是一种先进先出（First in first out，FIFO）的数据结构，而栈是种后进先出（Last in first out，LIFO）的数据结构。</p>
<img src="/notes/grokking-algorithms-bfs/1585582311086.png" class="" width="1585582311086">

<h3 id="Python-代码实现图"><a href="#Python-代码实现图" class="headerlink" title="Python 代码实现图"></a>Python 代码实现图</h3><p>接下来，我们要用 Python 代码实现图。</p>
<p>图由多个节点组成。节点之间有相互关联的关系。我们可以通过散列表来表示这种关系。</p>
<p>我们要将我们和朋友建立起一种映射关系。</p>
<img src="/notes/grokking-algorithms-bfs/1585582936747.png" class="" width="1585582936747">

<p>这种映射关系可以用散列表表示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">graph = &#123;&#125;</span><br><span class="line">graph[<span class="string">'you'</span>] = [<span class="string">'alice'</span>, <span class="string">'bob'</span>, <span class="string">'claire'</span>]</span><br></pre></td></tr></table></figure>

<p>图不过是一系列的节点和边，因此在 Python 中，只需使用上述代码就可表示一个图。</p>
<p>对于像下面这种比较复杂的图，我们同样可以采用上面的思路，只不过需要多写几个中间节点而已。</p>
<img src="/notes/grokking-algorithms-bfs/1585583225926.png" class="" width="1585583225926">

<p>用 Python 代码表示这个图中的关系就是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">graph = &#123;&#125;</span><br><span class="line">graph[<span class="string">'you'</span>] = [<span class="string">'alice'</span>, <span class="string">'bob'</span>, <span class="string">'claire'</span>]</span><br><span class="line">graph[<span class="string">'bob'</span>] = [<span class="string">'anuj'</span>, <span class="string">'peggy'</span>]</span><br><span class="line">graph[<span class="string">'alice'</span>] = [<span class="string">'peggy'</span>]</span><br><span class="line">graph[<span class="string">'claire'</span>] = [<span class="string">'thom'</span>, <span class="string">'jonny'</span>]</span><br><span class="line">graph[<span class="string">'anuj'</span>] = []</span><br><span class="line">graph[<span class="string">'peggy'</span>] = []</span><br><span class="line">graph[<span class="string">'thom'</span>] = []</span><br><span class="line">graph[<span class="string">'jonny'</span>] = []</span><br></pre></td></tr></table></figure>

<p>Anuj、 Peggy、Thom 和 Jonny 都没有邻居，这是因为虽然有指向他们的箭头，但没有从他们出发指向其他人的箭头。这被称为有向图（directed graph），其中的关系是单向的。因此，Anuj 是 Bob 的邻居，但 Bob 不是 Anuj 的邻居。无向图（undirected graph）没有箭头，直接相连的节点互为邻居。例如，下面两个图是等价的。</p>
<img src="/notes/grokking-algorithms-bfs/1585583539128.png" class="" width="1585583539128">

<h3 id="实现算法"><a href="#实现算法" class="headerlink" title="实现算法"></a>实现算法</h3><p>算法的工作原理为：</p>
<img src="/notes/grokking-algorithms-bfs/1585583593464.png" class="" width="1585583593464">

<p> 首先，创建一个队列。在 Python 中，可使用函数 deque 来创建一个双端队列：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line">search_queue = deque()    <span class="comment"># 创建一个队列</span></span><br><span class="line">search_queue += graph[<span class="string">'you'</span>]    <span class="comment"># 将你的邻居加入到队列中</span></span><br></pre></td></tr></table></figure>

<p>下面是查看朋友圈中是否有芒果经销商的完整函数代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">search</span><span class="params">(graph)</span>:</span></span><br><span class="line">    search_queue = deque()  <span class="comment"># 创建一个队列</span></span><br><span class="line">    search_queue += graph[<span class="string">'you'</span>]  <span class="comment"># 将你的邻居加入到队列中</span></span><br><span class="line">    <span class="keyword">while</span> search_queue:    <span class="comment"># 只要队列不为空</span></span><br><span class="line">        person = search_queue.popleft()    <span class="comment"># 就取出队列中的第一个人</span></span><br><span class="line">        <span class="keyword">if</span> person_is_seller(person):    <span class="comment"># 检查这个人是否是芒果经销商</span></span><br><span class="line">            print(person, <span class="string">'is a seller!'</span>)    <span class="comment"># 是芒果经销商</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            search_queue += graph[person]    <span class="comment"># 不是芒果经销商，把这个人的朋友圈都加入到队列中</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span>    <span class="comment"># 如果到达这里，就说明队列中没有芒果经销商</span></span><br></pre></td></tr></table></figure>

<p>我们可以定义一个判断一个人是否是芒果经销商的函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">person_is_seller</span><span class="params">(name)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> name[<span class="number">-1</span>] == <span class="string">'m'</span></span><br></pre></td></tr></table></figure>

<p>通过一个人名字是否是以 m 结尾来判断他是不是芒果经销商，这在现实中很荒谬，但是给我们做例子已经足够了。</p>
<p>下面是广度优先搜索的执行过程：</p>
<img src="/notes/grokking-algorithms-bfs/1585588829384.png" class="" width="1585588829384">

<p>这个算法将不断执行，直到满足以下条件之一：</p>
<ul>
<li>找到一位芒果销售商；</li>
<li>队列变成空的，这意味着你的人际关系网中没有芒果销售商。</li>
</ul>
<p>我们前面的代码并不完善，比如 Peggy 既是 Alie 的朋友又是 Bob 的朋友，因此她将被加入队列两次：一次是在添加 Alice 的朋友时，另一次是在添加 Bob 的朋友时。因此，搜索队列将包含两个 Peggy。</p>
<p>在这个问题中不会出现差错，但是对于有循环关系网的时候，我们有可能会陷入无限循环中。</p>
<img src="/notes/grokking-algorithms-bfs/1585589015841.png" class="" width="1585589015841">

<p>为了避免这种情况发生，我们需要限制，已经处检查的人将不会被重复检查。</p>
<p>这时，我们可以创建一个列表，专门存放已经检查过的人。如果下一个要检查的人出现在这个列表中，我们可以直接跳过，不去检查。</p>
<p>考虑到这一点，广度优先搜索算法的最终版本为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">graph = &#123;&#125;</span><br><span class="line">graph[<span class="string">'you'</span>] = [<span class="string">'alice'</span>, <span class="string">'bob'</span>, <span class="string">'claire'</span>]</span><br><span class="line">graph[<span class="string">'bob'</span>] = [<span class="string">'anuj'</span>, <span class="string">'peggy'</span>]</span><br><span class="line">graph[<span class="string">'alice'</span>] = [<span class="string">'peggy'</span>]</span><br><span class="line">graph[<span class="string">'claire'</span>] = [<span class="string">'thom'</span>, <span class="string">'jonny'</span>]</span><br><span class="line">graph[<span class="string">'anuj'</span>] = []</span><br><span class="line">graph[<span class="string">'peggy'</span>] = []</span><br><span class="line">graph[<span class="string">'thom'</span>] = []</span><br><span class="line">graph[<span class="string">'jonny'</span>] = []</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">person_is_seller</span><span class="params">(name)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> name[<span class="number">-1</span>] == <span class="string">'m'</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">search</span><span class="params">(graph, name=<span class="string">'you'</span>)</span>:</span></span><br><span class="line">    search_queue = deque()  <span class="comment"># 创建一个队列</span></span><br><span class="line">    search_queue += graph[name]  <span class="comment"># 将你的邻居加入到队列中</span></span><br><span class="line">    searched = []    <span class="comment"># 这个数组用于存放已经检查过的人</span></span><br><span class="line">    <span class="keyword">while</span> search_queue:    <span class="comment"># 只要队列不为空</span></span><br><span class="line">        person = search_queue.popleft()    <span class="comment"># 就取出队列中的第一个人</span></span><br><span class="line">        <span class="keyword">if</span> person <span class="keyword">not</span> <span class="keyword">in</span> searched:    <span class="comment"># 仅当这个人没有被检查的时候，才会被检查</span></span><br><span class="line">            <span class="keyword">if</span> person_is_seller(person):    <span class="comment"># 检查这个人是否是芒果经销商</span></span><br><span class="line">                print(person, <span class="string">'is a seller!'</span>)    <span class="comment"># 是芒果经销商</span></span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                search_queue += graph[person]    <span class="comment"># 不是芒果经销商，把这个人的朋友圈都加入到队列中</span></span><br><span class="line">                searched.append(person)    <span class="comment"># 将这个人标记为已经检查过</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span>    <span class="comment"># 如果到达这里，就说明队列中没有芒果经销商</span></span><br><span class="line"></span><br><span class="line">search(graph, <span class="string">'you'</span>)</span><br></pre></td></tr></table></figure>

<h3 id="运行时间"><a href="#运行时间" class="headerlink" title="运行时间"></a>运行时间</h3><p>如果你在你的整个人际关系网中搜索芒果销售商，就意味着你将沿每条边前行（记住，边是从一个人到另一个人的箭头或连接），因此运行时间至少为 $O(边数)$。你还使用了一个队列，其中包含要检查的每个人。将一个人添加到队列需要的时间是固定的，即为 $O(1)$，因此对每个人都这样做需要的总时间为 $O(人数)$。所以，广度优先搜索的运行时间为 $O(人数+边数)$，这通常写作 $O(V+E)$，其中 V 为顶点（vertice）数，E 为边数。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sliu.vip/crawler/async-crawler/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="刘硕">
      <meta itemprop="description" content="不成为自己讨厌的人">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="刘硕的技术查阅手册">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/crawler/async-crawler/" class="post-title-link" itemprop="url">异步爬虫</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-03-30 00:05:29 / 修改时间：00:05:32" itemprop="dateCreated datePublished" datetime="2020-03-30T00:05:29+08:00">2020-03-30</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%88%AC%E8%99%AB/" itemprop="url" rel="index"><span itemprop="name">爬虫</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/crawler/async-crawler/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/crawler/async-crawler/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="异步爬虫概述"><a href="#异步爬虫概述" class="headerlink" title="异步爬虫概述"></a>异步爬虫概述</h3><p>异步爬虫的作用很显而易见，就是为了提高我们爬虫的效率。因为网络请求通常会消耗一点时间，普通的爬虫在网络请求这段时间会诸塞住，CPU 的资源是浪费掉了。使用异步爬虫，就是在一个任务请求数据的时候，把 CPU 让出来，处理其他任务，从而提高爬虫的效率。</p>
<p>异步爬虫在实际应用中很有用也很没有用。说他有用，是因为就是像前面说的，可以提高爬虫的效率。说他没用，是因为实际的爬虫中，我们很多时候不那么追求效率。毕竟如果请求过于频繁，是有可能把人家的服务器搞崩掉的，会造成麻烦。另外，如果人家的服务器设置了反爬，有可能会禁掉我们的 IP。所以有些时候，我们不但不会想方设法提高效率，反而还会适当降低一点请求的频率。细水长流嘛。</p>
<p>当然，话说回来，有没有用是他的事，学不学是我们的事。知道怎么进行异步爬虫，才不至于将来用的时候手忙脚乱。</p>
<p>在这里主要介绍两种异步爬虫机制：</p>
<ul>
<li>基于线程池的异步爬虫</li>
<li>基于单线程 + 多任务的异步爬虫（基于 asyncio 协程）</li>
</ul>
<p>当然异步爬虫不仅限于这两个，而且线程池和协程也会有很多种写法，这里只是抛砖引玉，具体怎么用还得随机应变，灵活使用。</p>
<h3 id="使用-Flask-框架搭建一个简单的-Web-服务器"><a href="#使用-Flask-框架搭建一个简单的-Web-服务器" class="headerlink" title="使用 Flask 框架搭建一个简单的 Web 服务器"></a>使用 Flask 框架搭建一个简单的 Web 服务器</h3><p>我们要进行异步爬虫的演示，要看到效果就要对比同步和异步过程需要消耗的时间。如果使用网络中的服务器，测试时间会受网络波动的影响，不一定准确。所以在开始介绍异步爬虫之前，先让我们在本地搭建一个 Web 服务器。这样，用这个服务器测试我们的结果，就不会受网速的影响了。</p>
<p>我们从前一致用 Django 搭建 Web 服务器，这次我们换一个轻量的 Web 框架，Flask。</p>
<p>首先，我们要安装 Flask：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install flask</span><br></pre></td></tr></table></figure>

<p>然后创建一个 py 文件，名字随意，比如就叫 <code>FlaskServer.py</code>。</p>
<p>然后就可以创建一个 Web 服务器了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, render_template</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> sleep</span><br><span class="line"><span class="comment"># 实例化一个app</span></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建视图函数&amp;路由地址</span></span><br><span class="line"><span class="meta">@app.route('/sure')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index_1</span><span class="params">()</span>:</span></span><br><span class="line">    sleep(<span class="number">2</span>)    <span class="comment"># 这里睡2秒是为了模拟网络请求消耗的时间</span></span><br><span class="line">    <span class="keyword">return</span> render_template(<span class="string">'test.html'</span>)    <span class="comment"># test.html放在FlaskServer.py同级目录下的templates文件夹中</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/xiaoming')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index_2</span><span class="params">()</span>:</span></span><br><span class="line">    sleep(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> render_template(<span class="string">'test.html'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/xiaohui')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index_3</span><span class="params">()</span>:</span></span><br><span class="line">    sleep(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> render_template(<span class="string">'test.html'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># debug=True表示开启调试模式：服务器端代码被修改后按下保存键会自动重启服务</span></span><br><span class="line">    app.run(debug=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>执行文件，即可启动 Flask 项目。项目默认运行在 <code>http://127.0.0.1:5000/</code>。</p>
<p>可以通过修改 run 的 port 的参数，置顶端口。</p>
<h3 id="基于线程池的异步爬虫"><a href="#基于线程池的异步爬虫" class="headerlink" title="基于线程池的异步爬虫"></a>基于线程池的异步爬虫</h3><p>首先，我们用从前的方法，写一个同步的爬虫代码，爬取我们刚刚写好的三个页面，看一看需要花费多长时间：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url_list = [</span><br><span class="line">    <span class="string">'http://127.0.0.1:5000/sure'</span>,</span><br><span class="line">    <span class="string">'http://127.0.0.1:5000/xiaoming'</span>,</span><br><span class="line">    <span class="string">'http://127.0.0.1:5000/xiaohui'</span>,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_requests</span><span class="params">(url)</span>:</span></span><br><span class="line">    page_text = requests.get(url).text</span><br><span class="line">    <span class="keyword">return</span> len(page_text)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    start = time.time()</span><br><span class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> url_list:</span><br><span class="line">        ret = get_requests(url)</span><br><span class="line">        print(ret)</span><br><span class="line">    print(<span class="string">'总耗时：'</span>, time.time() - start)</span><br></pre></td></tr></table></figure>

<p>运行代码，我们发现，总耗时大约是 6 秒。因为每个视图函数，我们都休眠 2 秒，用来模拟网络请求的消耗时间。3 个网页总共耗时 6 秒，没毛病。</p>
<p>接下来，我们使用线程池实现异步爬虫，看看会不会提高我们的爬虫效率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> multiprocessing.dummy <span class="keyword">import</span> Pool</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url_list = [</span><br><span class="line">    <span class="string">'http://127.0.0.1:5000/sure'</span>,</span><br><span class="line">    <span class="string">'http://127.0.0.1:5000/xiaoming'</span>,</span><br><span class="line">    <span class="string">'http://127.0.0.1:5000/xiaohui'</span>,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_requests</span><span class="params">(url)</span>:</span></span><br><span class="line">    page_text = requests.get(url).text</span><br><span class="line">    <span class="keyword">return</span> len(page_text)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    start = time.time()</span><br><span class="line">    pool = Pool(<span class="number">3</span>)</span><br><span class="line">    ret = pool.map(get_requests, url_list)</span><br><span class="line">    print(ret)</span><br><span class="line">    print(<span class="string">'总耗时：'</span>, time.time() - start)</span><br></pre></td></tr></table></figure>

<p>总耗时降低到了 2 秒左右，鸡蛋缩短了爬虫所需要的时间。</p>
<p>这里我们简单提一下 pool 线程池对象的 <code>map(callback,alist)</code> 方法。</p>
<ul>
<li>可以使用 callback 对 alist 中的每一个元素进行指定形式的异步操作</li>
</ul>
<h3 id="基于单线程多任务（协程）的异步爬虫"><a href="#基于单线程多任务（协程）的异步爬虫" class="headerlink" title="基于单线程多任务（协程）的异步爬虫"></a>基于单线程多任务（协程）的异步爬虫</h3><p>首先安装 asyncio 模块：</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> asyncio</span><br></pre></td></tr></table></figure>

<p>然后就要介绍几个概念：</p>
<h4 id="特殊的函数"><a href="#特殊的函数" class="headerlink" title="特殊的函数"></a>特殊的函数</h4><p>一个函数的定义被 async 修饰后，该函数就变成了一个特殊的函数</p>
<p>这个函数的特殊之处在于：</p>
<ul>
<li>该特殊的函数被调用后，函数中的代码不会被立即执行</li>
<li>该特殊函数被调用后的返回值是一个<strong>协程对象</strong></li>
</ul>
<h4 id="协程对象"><a href="#协程对象" class="headerlink" title="协程对象"></a>协程对象</h4><p>协程对象首先是一个对象。通过特殊函数的调用能够返回一个协程对象。</p>
<p>换句话说，协程对象就是特殊函数，而特殊函数是一组指定的操作。</p>
<p>那么，协程对象其实就是一组指定的操作。</p>
<h4 id="任务对象"><a href="#任务对象" class="headerlink" title="任务对象"></a>任务对象</h4><p>任务对象就是一个高级的协程对象（任务对象就是对协程对象的进一步封装）。</p>
<p>任务对象就是协程对象，而协程对象是特殊函数，特殊函数是封装了的一组指定操作。</p>
<p>所以，任务对象也是一组指定的操作。</p>
<p>创建任务对象的方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">asyncio.ensure_future(协程对象)</span><br></pre></td></tr></table></figure>

<p>任务对象的高级之处在于可以给任务对象绑定回调，这个特点在爬虫任务中，应用十分广泛。回调任务对象的方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">task.add_done_callback(task_callback)</span><br></pre></td></tr></table></figure>

<p>回调函数的调用时机：</p>
<ul>
<li>当任务执行完成后，才会调用回调函数</li>
</ul>
<p>回调函数的参数只可以有一个：表示的就是该回调函数的调用者（任务对象）</p>
<p>使用回调函数的参数调用 <code>result()</code> 方法，得到的就是任务对象表示的特殊函数 return 的结果</p>
<h4 id="事件循环对象"><a href="#事件循环对象" class="headerlink" title="事件循环对象"></a>事件循环对象</h4><ul>
<li><p>事件循环对象也是一个对象。</p>
</li>
<li><p>作用：</p>
<ul>
<li>可以将多个任务对象注册/装载到事件循环对象中</li>
<li>如果开启了事件循环后，则其内部注册/装载的任务对象表示的指定操作就会被基于异步的被执行</li>
</ul>
</li>
<li><p>创建事件循环对象：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loop = asyncio.get_event_loop()</span><br></pre></td></tr></table></figure>
</li>
<li><p>注册并启动事件循环对象：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loop.run_until_complete(task)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>启动事件循环对象，函数中的代码即可执行。</p>
<p>前面的几个概念，包括特殊的函数、协程对象、任务对象和事件循环的对象的用法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="comment"># 声明特殊的函数</span></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">get_request</span><span class="params">(url)</span>:</span></span><br><span class="line">    print(<span class="string">'正在请求的 url：'</span>, url)</span><br><span class="line">    time.sleep(<span class="number">2</span>)</span><br><span class="line">    print(<span class="string">'请求结束'</span>, url)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># 特殊的函数执行后，成为协程对象</span></span><br><span class="line">    c = get_request(<span class="string">'www.baidu.com'</span>)</span><br><span class="line">    <span class="comment"># 将协程对象转换为任务对象</span></span><br><span class="line">    task = asyncio.ensure_future(c)</span><br><span class="line">    <span class="comment"># 创建任务循环对象</span></span><br><span class="line">    loop = asyncio.get_event_loop()</span><br><span class="line">    <span class="comment"># 注册并启动事件循环对象</span></span><br><span class="line">    loop.run_until_complete(task)</span><br></pre></td></tr></table></figure>

<h4 id="wait-方法"><a href="#wait-方法" class="headerlink" title="wait 方法"></a>wait 方法</h4><p>wait 方法用来将任务列表中的任务对象赋予可被挂起的权限。只有任务对象被赋予了可被挂起的权限后，该<br>任务对象才可以被挂起。</p>
<p>挂起指的是，将当前的任务对象交出 CPU 的使用权。</p>
<p>wait 方法的作用如图所示：</p>
<img src="/crawler/async-crawler/Snip20200325_2.png" class="" title="Snip20200325_2">

<p>有了 wait 方法，我们就可以实现多任务操作了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">get_request</span><span class="params">(url)</span>:</span></span><br><span class="line">    print(<span class="string">'正在请求的 url：'</span>, url)</span><br><span class="line">    <span class="comment"># time.sleep是不支持异步模块的代码</span></span><br><span class="line">    time.sleep(<span class="number">2</span>)</span><br><span class="line">    print(<span class="string">'请求结束'</span>, url)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    url_list = [</span><br><span class="line">        <span class="string">'http://127.0.0.1:5000/sure'</span>,</span><br><span class="line">        <span class="string">'http://127.0.0.1:5000/xiaoming'</span>,</span><br><span class="line">        <span class="string">'http://127.0.0.1:5000/xiaohui'</span>,</span><br><span class="line">    ]</span><br><span class="line">    task_list = []</span><br><span class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> url_list:</span><br><span class="line">        c = get_request(url)</span><br><span class="line">        task = asyncio.ensure_future(c)</span><br><span class="line">        task_list.append(task)</span><br><span class="line">    loop = asyncio.get_event_loop()</span><br><span class="line">    <span class="comment"># 必须使用wait方法对task_list进行封装才可以实现多任务，wait赋予task_list中所有的任务可挂起的权限</span></span><br><span class="line">    loop.run_until_complete(asyncio.wait(task_list))</span><br></pre></td></tr></table></figure>

<p>我们看到，代码顺利执行了，也没报错。但总觉得哪里不对——说好的异步执行呢？三个任务逐个完成，分明是串行的，没有一点异步执行的意味。</p>
<img src="/crawler/async-crawler/multitaskingfail.gif" class="" title="multitaskingfail">

<p>注意事项【重要】：</p>
<ul>
<li>在特殊函数内部不可以出现不支持异步模块对应的代码，否则会中断整个异步效果</li>
</ul>
<p>很遗憾，<code>time.sleep</code> 和 <code>requests</code> 都是不支持异步的模块，所以都不能用在这里。</p>
<p>我们需要把 <code>time.sleep</code> 更换为 <code>asyncio.sleep</code>，把 <code>requests</code> 更换为我们下面很快就会讨论的 <code>aiohttp</code>。</p>
<h4 id="await-关键字"><a href="#await-关键字" class="headerlink" title="await 关键字"></a>await 关键字</h4><p>前面的代码光把不支持异步的模块更换为支持的模块还是不够的，我们还需要使用 await 关键字。否则的话，阻塞的操作会被直接跳过，不被执行。</p>
<p>在特殊函数内部，凡是阻塞操作前都必须使用 await 进行修饰。await 就可以保证阻塞操作在异步执行的过程中不会被跳过！</p>
<p>于是，<strong>完整的多任务操作</strong>代码就是这样的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">get_request</span><span class="params">(url)</span>:</span></span><br><span class="line">    print(<span class="string">'正在请求的 url：'</span>, url)</span><br><span class="line">    <span class="comment"># 使用支持异步的阻塞模块，且用await装饰</span></span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">2</span>)</span><br><span class="line">    print(<span class="string">'请求结束'</span>, url)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    url_list = [</span><br><span class="line">        <span class="string">'http://127.0.0.1:5000/sure'</span>,</span><br><span class="line">        <span class="string">'http://127.0.0.1:5000/xiaoming'</span>,</span><br><span class="line">        <span class="string">'http://127.0.0.1:5000/xiaohui'</span>,</span><br><span class="line">    ]</span><br><span class="line">    task_list = []</span><br><span class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> url_list:</span><br><span class="line">        c = get_request(url)</span><br><span class="line">        task = asyncio.ensure_future(c)</span><br><span class="line">        task_list.append(task)</span><br><span class="line">    loop = asyncio.get_event_loop()</span><br><span class="line">    loop.run_until_complete(asyncio.wait(task_list))</span><br></pre></td></tr></table></figure>

<p>就成功实现异步操作了</p>
<img src="/crawler/async-crawler/multitaskingsuccess.gif" class="" title="multitaskingsuccess">

<h4 id="aiohttp-异步请求模块"><a href="#aiohttp-异步请求模块" class="headerlink" title="aiohttp 异步请求模块"></a>aiohttp 异步请求模块</h4><p>我们前面提到，requests 并不是一个支持异步的模块。若要实现异步爬虫，我们就需要使用 aiohttp。</p>
<p>aiohttp 是一个支持异步的网络请求模块。</p>
<p>aiohttp 模块的安装：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install aiohttp</span><br></pre></td></tr></table></figure>

<p>使用代码：</p>
<ol>
<li><p>写出一个大致的架构：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> aiohttp</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">get_request</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="comment"># 实例化一个请求对象，使用with是为了避免后续关闭起来混乱</span></span><br><span class="line">    <span class="keyword">with</span> aiohttp.ClientSession() <span class="keyword">as</span> sess:</span><br><span class="line">        <span class="comment"># 调用get发起请求，返回一个响应对象</span></span><br><span class="line">        <span class="comment"># get/post(url,headers,params/data,proxy="http://ip:port")</span></span><br><span class="line">        <span class="keyword">with</span> sess.get(url=url) <span class="keyword">as</span> response:</span><br><span class="line">            <span class="comment"># 获取字符串形式的响应数据</span></span><br><span class="line">            page_text = response.text()</span><br><span class="line">            <span class="keyword">return</span> page_text</span><br></pre></td></tr></table></figure>
</li>
<li><p>补充细节</p>
<ul>
<li>在阻塞操作前加上 await 关键字。对于网络请求来说，发送请求和获取数据都会有阻塞</li>
<li>在每一个 with 前加上 async 关键字</li>
</ul>
</li>
</ol>
<p>完整代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> aiohttp</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">get_request</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="comment"># 实例化一个请求对象，使用with是为了避免后续关闭起来混乱</span></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> aiohttp.ClientSession() <span class="keyword">as</span> sess:</span><br><span class="line">        <span class="comment"># 调用get发起请求，返回一个响应对象</span></span><br><span class="line">        <span class="comment"># get/post(url,headers,params/data,proxy="http://ip:port")</span></span><br><span class="line">        <span class="keyword">async</span> <span class="keyword">with</span> <span class="keyword">await</span> sess.get(url=url) <span class="keyword">as</span> response:</span><br><span class="line">            <span class="comment"># text()获取字符串形式的响应数据</span></span><br><span class="line">            <span class="comment"># read()获取byte类型的响应数据</span></span><br><span class="line">            <span class="comment"># json()获取json类型的数据</span></span><br><span class="line">            page_text = <span class="keyword">await</span> response.text()</span><br><span class="line">            <span class="keyword">return</span> page_text</span><br></pre></td></tr></table></figure>

<h4 id="多任务爬虫完整代码"><a href="#多任务爬虫完整代码" class="headerlink" title="多任务爬虫完整代码"></a>多任务爬虫完整代码</h4><p>多任务爬虫的数据解析一定要使用任务对象的回调函数实现数据解析，因为多任务的架构中数据的爬取是封装在特殊函数中，我们一定要保证数据请求结束后，再实现数据解析。</p>
<p>使用多任务的异步协程爬取数据实现套路：</p>
<ul>
<li>先使用 requests 模块将待请求数据对应的 url 封装到某个列表中（同步）</li>
<li>然后使用 aiohttp 模式将列表中的 url 进行异步的请求和数据解析（异步）</li>
</ul>
<p>多任务爬虫完整代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> aiohttp</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">get_request</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="comment"># 实例化一个请求对象，使用with是为了避免后续关闭起来混乱</span></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> aiohttp.ClientSession() <span class="keyword">as</span> sess:</span><br><span class="line">        <span class="comment"># 调用get发起请求，返回一个响应对象</span></span><br><span class="line">        <span class="comment"># get/post(url,headers,params/data,proxy="http://ip:port")</span></span><br><span class="line">        <span class="keyword">async</span> <span class="keyword">with</span> <span class="keyword">await</span> sess.get(url=url) <span class="keyword">as</span> response:</span><br><span class="line">            <span class="comment"># text()获取字符串形式的响应数据</span></span><br><span class="line">            <span class="comment"># read()获取byte类型的响应数据</span></span><br><span class="line">            <span class="comment"># json()获取json类型的数据</span></span><br><span class="line">            page_text = <span class="keyword">await</span> response.text()</span><br><span class="line">            <span class="keyword">return</span> page_text</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解析函数的封装</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(task)</span>:</span></span><br><span class="line">    <span class="comment"># 获取请求到页面源码数据</span></span><br><span class="line">    page_text = task.result()</span><br><span class="line">    tree = etree.HTML(page_text)</span><br><span class="line">    msg = tree.xpath(<span class="string">'//a[@id="feng"]//text()'</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 数据持久化</span></span><br><span class="line">    print(msg)</span><br><span class="line"></span><br><span class="line">url_list = [</span><br><span class="line">    <span class="string">'http://127.0.0.1:5000/sure'</span>,</span><br><span class="line">    <span class="string">'http://127.0.0.1:5000/xiaoming'</span>,</span><br><span class="line">    <span class="string">'http://127.0.0.1:5000/xiaohui'</span>,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    start = time.time()</span><br><span class="line">    task_list = []</span><br><span class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> url_list:</span><br><span class="line">        task = asyncio.ensure_future(get_request(url))</span><br><span class="line">        task.add_done_callback(parse)</span><br><span class="line">        task_list.append(task)</span><br><span class="line">    loop = asyncio.get_event_loop()</span><br><span class="line">    loop.run_until_complete(asyncio.wait(task_list))</span><br><span class="line">    print(<span class="string">'总耗时：'</span>, time.time() - start)</span><br></pre></td></tr></table></figure>

<h3 id="简答题"><a href="#简答题" class="headerlink" title="简答题"></a>简答题</h3><ol>
<li><p>说明是特殊函数特殊之处在哪里</p>
<ul>
<li><p>定义特殊：需要用 async 修饰</p>
</li>
<li><p>执行过程特殊：执行后，函数中的代码不会立即执行</p>
</li>
<li><p>返回值特殊：返回的是协程对象</p>
</li>
</ul>
</li>
<li><p>阐述任务对象和协程的区别，任务对象的回调函数在爬虫中的作用是什么</p>
<p>任务对象和协程对象都是函数中封装的一组可执行的操作。任务对象是对协程对象的进一步封装，可以实现结果的回调。</p>
<p>在爬虫中，请求任务的返回结果往往还需要做进一步的处理。这些处理，需要拿到结果后才能进行。所以，最好的办法是将数据处理的代码作为回调函数交给任务。当任务执行完毕，返回页面数据。回调函数就可以用来处理页面数据了。</p>
</li>
<li><p>阐述事件循环的工作流程</p>
<p>首先，实例化一个事件循环对象：<code>loop = asyncio.get_event_loop()</code></p>
<p>然后，注册并执行实现循环对象即可：<code>loop.run_until_complete(tast)</code></p>
</li>
<li><p>简述 wait() 和 await 关键字的作用</p>
<p>wait 可以实现列表中的多个任务的并发执行。当执行中的任务进入阻塞时，将变成挂起状态，交出 CPU 的使用权。待其完成阻塞，将继续执行。</p>
<p>await 用来保证阻塞的操作在异步任务中不被跳过。</p>
</li>
<li><p>aiohttp的基本使用有哪些？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 伪代码</span></span><br><span class="line"><span class="keyword">with</span> aiohttp.ClientSession <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="keyword">with</span> sess.get/post(url=url, headers=headers, data=data, params=params, proxy=<span class="string">'代理'</span>) <span class="keyword">as</span> response:</span><br><span class="line">        <span class="keyword">return</span> response.text()/read()</span><br></pre></td></tr></table></figure>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sliu.vip/crawler/scrapy-basic/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="刘硕">
      <meta itemprop="description" content="不成为自己讨厌的人">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="刘硕的技术查阅手册">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/crawler/scrapy-basic/" class="post-title-link" itemprop="url">scrapy 框架的基本用法</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-03-30 00:05:29 / 修改时间：00:05:32" itemprop="dateCreated datePublished" datetime="2020-03-30T00:05:29+08:00">2020-03-30</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%88%AC%E8%99%AB/" itemprop="url" rel="index"><span itemprop="name">爬虫</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/crawler/scrapy-basic/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/crawler/scrapy-basic/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="scrapy-简要介绍"><a href="#scrapy-简要介绍" class="headerlink" title="scrapy 简要介绍"></a>scrapy 简要介绍</h3><p>我们从前接触到过 DJango，是一个 Web 框架。那么什么是框架呢？</p>
<p>所谓的框架，其实就是一个被集成了很多功能且具有很强通用性的一个项目模板。</p>
<p>对于框架的学习，往往会经历两个阶段：</p>
<ul>
<li>初级阶段，学习框架中集成好的各种功能的特性及作用，也就是知道怎么用</li>
<li>进阶阶段，逐步的探索框架的底层，知道为什么要这么用，进而知道该如何实现更高级的功能</li>
</ul>
<p>我们写爬虫代码，需要经常写一些请求发送、数据解析、存储数据的代码。重复写代码当然不是一个好事情。同时，如果我们想要对爬虫代码进行优化，又要付出很大精力写很多代码才行。有时候，受限于自身水平，麻麻烦烦写好的优化代码效率仍然不让人满意。于是，scrapy 应运而生。</p>
<p>scrapy 是一个专门用于异步爬虫的框架，可以实现高性能的数据解析、请求发送、持久化存储、全站数据爬取，中间件、分布式……</p>
<h3 id="scrapy-环境的安装"><a href="#scrapy-环境的安装" class="headerlink" title="scrapy 环境的安装"></a>scrapy 环境的安装</h3><p>macOS 和 Linux 系统可以直接 pip 安装：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install scrapy</span><br></pre></td></tr></table></figure>

<p>Windows 系统的安装要稍微繁琐些，需要先安装 Twisted，然后才能使用 pip 安装 scrapy。Twisted 是一个网络引擎框架。scrapy 需要依赖 Twisted 环境。对于 macOS 和 Linux 系统来说，会自动安装 Twisted，但是 Windows 系统并不能自动安装，所以需要我们手动下载配置。</p>
<ol>
<li><p>安装 wheel，wheel 可以安装离线的 Python 包：</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 <span class="keyword">install</span> wheel</span><br></pre></td></tr></table></figure>
</li>
<li><p>下载 twisted 文件，下载地址：<a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted" target="_blank" rel="noopener">http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted</a></p>
<p>找到指定版本的 twisted 文件，点击即可下载，比如我的 Python 版本是 3.6.8，操作系统是 64 位的，就选择这个：</p>
<img src="/crawler/scrapy-basic/1585297853689.png" class="" width="1585297853689">
</li>
<li><p>进入下载目录，执行命令安装 Twisted（注意文件名要是刚刚下载的 Twisted 安装文件）：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install Twisted‑20.3.0‑cp36‑cp36m‑win_amd64.whl</span><br></pre></td></tr></table></figure>

<p>Twisted 就是一个异步的架构，被作用在了 scrapy 中。</p>
<p>如果安装报错，可以尝试更换另一个版本的 Twisted 文件进行安装。</p>
</li>
<li><p>安装 pywin32：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pywin32</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装 scrapy：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install scrapy</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>测试：cmd 中输入 scrapy 按下回车，如果没有报错说明安装成功。</p>
<img src="/crawler/scrapy-basic/1585298363447.png" class="" width="1585298363447">

<h3 id="scrapy-工程创建"><a href="#scrapy-工程创建" class="headerlink" title="scrapy 工程创建"></a>scrapy 工程创建</h3><p>在终端中输入命令，创建一个 scrapy 的工程项目：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject ProName</span><br></pre></td></tr></table></figure>

<p>输入完上面的命令，会在当前目录下生成 scrapy 项目文件夹。文件夹的目录结构如下：</p>
<img src="/crawler/scrapy-basic/1585300512681.png" class="" width="1585300512681">

<p>这些文件和文件夹，我们后面用到的时候会详细讨论，先简单介绍两个：</p>
<ul>
<li>spiders 文件夹，是爬虫文件夹，用来存放爬虫源文件。</li>
<li>settings.py 文件，是工程项目的配置文件</li>
</ul>
<h3 id="创建爬虫源文件"><a href="#创建爬虫源文件" class="headerlink" title="创建爬虫源文件"></a>创建爬虫源文件</h3><p>进入到项目文件夹中，运行命令创建爬虫源文件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ProName</span><br><span class="line">scrapy genspider spiderName www.xxx.com</span><br></pre></td></tr></table></figure>

<p>运行成功后，将会在 spiders 文件夹中生成一个爬虫源文件，长这样：</p>
<img src="/crawler/scrapy-basic/1585312524627.png" class="" width="1585312524627">

<p>我们就可以在这个文件中编写爬虫代码了。比如写成这样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FirstSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    <span class="comment"># 爬虫文件名称：当前源文件的唯一标识</span></span><br><span class="line">    name = <span class="string">'first'</span></span><br><span class="line">    <span class="comment"># 允许的域名，一般不用，会注释掉</span></span><br><span class="line">    <span class="comment"># allowed_domains = ['www.xxx.com']</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 起始的url列表：只可以存储url</span></span><br><span class="line">    <span class="comment"># 作用：列表中存储的url都会被进行get请求的发送</span></span><br><span class="line">    start_urls = [<span class="string">'http://www.baidu.com/'</span>, <span class="string">'http://www.sogou.com/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据解析</span></span><br><span class="line">    <span class="comment"># parse方法调用的次数完全取决于请求的次数</span></span><br><span class="line">    <span class="comment"># 参数response：表示的就是服务器返回的响应对象</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        print(response)</span><br></pre></td></tr></table></figure>

<p>爬虫文件 spiderName 内容阐述：</p>
<ul>
<li>name：爬虫文件名称，该文件的唯一标识</li>
<li>start_urls：起始 url 列表，存储的都是 url，url 可以被自动进行 get 请求的发送</li>
<li>parse 方法：请求后的数据解析操作</li>
</ul>
<h3 id="执行工程和-settings-py-文件的配置"><a href="#执行工程和-settings-py-文件的配置" class="headerlink" title="执行工程和 settings.py 文件的配置"></a>执行工程和 settings.py 文件的配置</h3><p>使用命令执行 scrapy 工程：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl spiderName</span><br></pre></td></tr></table></figure>

<p>工程启动了吗？启动了，因为屏幕有输出显示内容来。但是看不懂，而且太乱：</p>
<img src="/crawler/scrapy-basic/scrapyruncrawllog-1585313900790.gif" class="" title="scrapyruncrawllog-1585313900790">

<p>这是因为，执行工程后，默认会输出工程所有的日志信息。我们看到的满屏看不懂的东西，就是项目执行的日志记录。</p>
<p>但是我们往往并不希望看到这些日志。这些 info 级别的日志，不看也罢。</p>
<p>我们可以在 <code>settings.py</code> 文件中加入下面的配置来指定类型日志的输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LOG_LEVEL = <span class="string">'ERROR'</span></span><br></pre></td></tr></table></figure>

<p>再次执行工程，日志确实没有了，但是我们要打印的东西也没打印出来。事实上，什么都没有显示出来：</p>
<img src="/crawler/scrapy-basic/1585318654991.png" class="" width="1585318654991">

<p>没有打印结果，说明我们的 parse 方法没有被执行。为什么没有被执行呢？</p>
<p>原来，scrapy 框架默认会遵守网站的 robots 协议。很显然，我们的网址都是在 robots 协议中禁止爬取的。</p>
<p>我们当然可以放弃爬取，但你一定不愿意这么做——如果严格遵守 robots 协议，网络中几乎什么数据都爬取不到了。我们可以通过配置，来无视掉网站的 robots 协议。</p>
<p>在 settings.py 中，修改 ROBOTSTXT_OBEY 的配置位 False：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br></pre></td></tr></table></figure>

<p>再次执行工程，成功打印出 parse 方法的参数，response 的值，为两个对象：</p>
<img src="/crawler/scrapy-basic/1585319121091.png" class="" width="1585319121091">

<p>除了配置 robots 协议和日志外，我们还可以指定 user-agent 进行 UA 伪装。在设置中找到 User-Agent，取消注释，然后把 user-agent 配置过去即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">USER_AGENT = <span class="string">'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36'</span></span><br></pre></td></tr></table></figure>

<p>小结一下，目前我们可以通过 <code>settings.py</code> 进行三个配置：</p>
<ul>
<li>无视 robots</li>
<li>指定日志类型：<code>LOG_LEVEL = &#39;ERROR&#39;</code></li>
<li>UA 伪装</li>
</ul>
<h3 id="scrapy-数据解析"><a href="#scrapy-数据解析" class="headerlink" title="scrapy 数据解析"></a>scrapy 数据解析</h3><p>scrapy 给我们提供了通过 xpath 解析数据的方法，我们只需要在 scrapy 爬虫源文件重点 parse 方法的 response 参数使用 xpath 解析即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">response.xpath(<span class="string">'xpath表达式'</span>)</span><br></pre></td></tr></table></figure>

<p>比如，爬取段子王网站的经典段子页面（<code>https://duanziwang.com/category/经典段子/</code>）的段子内容，可以这样爬取解析：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DuanziSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'duanzi'</span></span><br><span class="line">    <span class="comment"># allowed_domains = ['https://duanziwang.com/category/经典段子/']</span></span><br><span class="line">    start_urls = [<span class="string">'https://duanziwang.com/category/经典段子/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据解析</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment"># 数据解析标题和内容</span></span><br><span class="line">        article_list = response.xpath(<span class="string">'//main/article'</span>)</span><br><span class="line">        <span class="keyword">for</span> article <span class="keyword">in</span> article_list:</span><br><span class="line">            title = article.xpath(<span class="string">'./div[1]/h1/a/text()'</span>)[<span class="number">0</span>]</span><br><span class="line">            content = article.xpath(<span class="string">'./div[2]/p/text()'</span>)[<span class="number">0</span>]</span><br><span class="line">            print(title, content)</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<p>按照 lxml 模块的 etree 的 xpath 解析的话，title 和 content 应该是字符串。但是从打印出来的结果来看，我们获取到的却是 Selector 对象，字符串数据存放在对象的 data 属性中：</p>
<img src="/crawler/scrapy-basic/1585322099780.png" class="" width="1585322099780">

<p>若要拿到对象的 data 属性，我们只需调用对象的 extract 方法即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">title = article.xpath(<span class="string">'./div[1]/h1/a/text()'</span>)[<span class="number">0</span>].extract()</span><br><span class="line">content = article.xpath(<span class="string">'./div[2]/p/text()'</span>)[<span class="number">0</span>].extract()</span><br></pre></td></tr></table></figure>

<img src="/crawler/scrapy-basic/1585322761694.png" class="" width="1585322761694">

<p>但是上面那种，拿到 Selector 对象再使用 extract 方法提取数据的方式并不常用。我们更常用的是直接对列表元素进行取值操作。</p>
<p>比如，使用 extract_first 方法可以提取列表中第一个 Selector 对象中的 data 数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">title = article.xpath(<span class="string">'./div[1]/h1/a/text()'</span>).extract_first()</span><br><span class="line">content = article.xpath(<span class="string">'./div[2]/p/text()'</span>).extract_first()</span><br></pre></td></tr></table></figure>

<img src="/crawler/scrapy-basic/1585322775292.png" class="" width="1585322775292">

<p>我们也可以对列表直接使用 extract 方法，提取到里面每一个 Selector 的 data 值。返回的数据也是列表：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">title = article.xpath(<span class="string">'./div[1]/h1/a/text()'</span>).extract()</span><br><span class="line">content = article.xpath(<span class="string">'./div[2]/p/text()'</span>).extract()</span><br></pre></td></tr></table></figure>

<img src="/crawler/scrapy-basic/1585322871413.png" class="" width="1585322871413">

<p>总结一下 scrapy 封装的 xpath 和 etree 中的 xpath 区别：</p>
<ul>
<li>scrapy 中的 xpath 直接将定位到的标签中存储的值或者属性值取出，返回的是 Selector 对象，且相关<br>的数据值是存储在 Selector 对象的 data 属性中，需要调用 extract、extract_first() 取出字符串数据</li>
</ul>
<h3 id="数据持久化存储"><a href="#数据持久化存储" class="headerlink" title="数据持久化存储"></a>数据持久化存储</h3><p>我们爬取到的数据，最开始是储存到内存中。我们前面的例子，是将数据打印出来。但是这显然是不够的——如果我们关闭了窗口，数据将不复存在。数据分析也无从谈起。</p>
<p>我们需要通过持久化存储，将数据保存到硬盘中。或是以文件形式，或是存储到数据库中。这样我们将来就可以对数据进行查看和分析了。</p>
<h4 id="基于终端指令的持久化存储"><a href="#基于终端指令的持久化存储" class="headerlink" title="基于终端指令的持久化存储"></a>基于终端指令的持久化存储</h4><p>首先，要将我们需要存储的数据在 parse 中整理并返回：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DuanziSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'duanzi'</span></span><br><span class="line">    <span class="comment"># allowed_domains = ['https://duanziwang.com/category/经典段子/']</span></span><br><span class="line">    start_urls = [<span class="string">'https://duanziwang.com/category/经典段子/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将解析到的数据进行持久化存储:基于终端指令的持久化存储</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment"># 数据解析标题和内容</span></span><br><span class="line">        article_list = response.xpath(<span class="string">'//main/article'</span>)</span><br><span class="line">        data = []</span><br><span class="line">        <span class="keyword">for</span> article <span class="keyword">in</span> article_list:</span><br><span class="line">            title = article.xpath(<span class="string">'./div[1]/h1/a/text()'</span>).extract_first()</span><br><span class="line">            content = article.xpath(<span class="string">'./div[2]/p/text()'</span>).extract_first()</span><br><span class="line">            data.append(&#123;</span><br><span class="line">                <span class="string">'title'</span>: title,</span><br><span class="line">                <span class="string">'content'</span>: content,</span><br><span class="line">            &#125;)</span><br><span class="line">        <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure>

<p>然后，在执行工程的指令后面加上 -o 参数指定输出的文件位置：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl spiderName -o filePath</span><br></pre></td></tr></table></figure>

<p>需要注意的是，该种方式只可以将 parse 方法的返回值存储到本地指定后缀（json、jsonlines、jl、csv、xml、marshal、pickle）的文本文件中。</p>
<h4 id="基于管道的持久化存储（重点）"><a href="#基于管道的持久化存储（重点）" class="headerlink" title="基于管道的持久化存储（重点）"></a>基于管道的持久化存储（重点）</h4><p>基于命令存储文件有两个主要的弊端：首先，文件必须是指定后缀名才可以；第二，只能保存到文件中，无法向数据库中存储。</p>
<p>好在 scrapy 给我们提供了管道，可以对数据进行任意的存储操作。文件可以是任意类型任意后缀，也可以把数据写入到数据库中。</p>
<p>首先，我们要在爬虫文件中进行数据解析，确定我们要保存的数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DuanziSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'duanzi'</span></span><br><span class="line">    <span class="comment"># allowed_domains = ['https://duanziwang.com/category/经典段子/']</span></span><br><span class="line">    start_urls = [<span class="string">'https://duanziwang.com/category/经典段子/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 基于管道的持久化存储</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment"># 数据解析标题和内容</span></span><br><span class="line">        article_list = response.xpath(<span class="string">'//main/article'</span>)</span><br><span class="line">        <span class="keyword">for</span> article <span class="keyword">in</span> article_list:</span><br><span class="line">            <span class="comment"># title和content是我们解析出来要保存的数据</span></span><br><span class="line">            title = article.xpath(<span class="string">'./div[1]/h1/a/text()'</span>).extract_first()</span><br><span class="line">            content = article.xpath(<span class="string">'./div[2]/p/text()'</span>).extract_first()</span><br></pre></td></tr></table></figure>

<p>然后，在 <code>items.py</code> 中定义相关属性。注意，我们需要保存哪些数据，就在这里定义对应的属性：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DuanziproItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    <span class="comment"># Filed()定义好的属性当做是一个万能类型的属性</span></span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    content = scrapy.Field()</span><br></pre></td></tr></table></figure>

<p>然后回到爬虫文件，导入我们刚刚写好的 Item 类。先实例化一个 Item 对象，然后把解析到的数据存储封装到 Item 类型的对象中。最后， 使用 yield 把 Item 类型的对象数据提交给管道：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> duanziPro.items <span class="keyword">import</span> DuanziproItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DuanziSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'duanzi'</span></span><br><span class="line">    <span class="comment"># allowed_domains = ['https://duanziwang.com/category/经典段子/']</span></span><br><span class="line">    start_urls = [<span class="string">'https://duanziwang.com/category/经典段子/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 基于管道的持久化存储</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment"># 数据解析标题和内容</span></span><br><span class="line">        article_list = response.xpath(<span class="string">'//main/article'</span>)</span><br><span class="line">        <span class="keyword">for</span> article <span class="keyword">in</span> article_list:</span><br><span class="line">            <span class="comment"># title和content是我们解析出来要保存的数据</span></span><br><span class="line">            title = article.xpath(<span class="string">'./div[1]/h1/a/text()'</span>).extract_first()</span><br><span class="line">            content = article.xpath(<span class="string">'./div[2]/p/text()'</span>).extract_first()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 实例化一个item类型的对象，将解析到的数据存储到该对象中</span></span><br><span class="line">            item = DuanziproItem()</span><br><span class="line">            <span class="comment"># 不可以通过.的形式调用属性</span></span><br><span class="line">            item[<span class="string">'title'</span>] = title</span><br><span class="line">            item[<span class="string">'content'</span>] = content</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 将item对象提交给管道</span></span><br><span class="line">            <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure>

<p>接下来，在管道文件 <code>pipelines.py</code> 中接收爬虫文件提交过来的 Item 类型对象，且对其进行任意形式的持久化存储操作。比如，将数据写入到本地文件中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DuanziproPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    fp = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 重写父类的两个方法，每次爬虫过程，只打开关闭文件一次，提高效率</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        print(<span class="string">'我是open_spider()，我只会在爬虫开始的时候执行一次！'</span>)</span><br><span class="line">        self.fp = open(<span class="string">'duanzi.txt'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        print(<span class="string">'我是close_spider()，我只会在爬虫结束的时候执行一次！'</span>)</span><br><span class="line">        self.fp.close()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 该方法是用来接收item对象。一次只能接收一个item，说明该方法会被调用多次</span></span><br><span class="line">    <span class="comment"># 参数item：就是接收到的item对象</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="comment"># print(item)    # item其实就是一个字典</span></span><br><span class="line">        self.fp.write(item[<span class="string">'title'</span>] + <span class="string">'：'</span> + item[<span class="string">'content'</span>] + <span class="string">'\n'</span>)</span><br><span class="line">        <span class="comment"># 将item存储到本文文件</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>

<p>然后，需要在配置文件中开启管道机制，将下面的代码取消注释：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">'duanziPro.pipelines.DuanziproPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这里的 300 指代的是管道的优先级。数值越小优先级越高，优先级高的管道类先被执行。</p>
<h4 id="基于管道实现数据的备份"><a href="#基于管道实现数据的备份" class="headerlink" title="基于管道实现数据的备份"></a>基于管道实现数据的备份</h4><p>数据备份，就是将数据分别存储到不同的载体中。</p>
<p>需求：将数据一份存储到 MySQL，一份存储到 Redis</p>
<p>问题：管道文件中的一个管道类表示怎样的一组操作呢？</p>
<p>一个管道类对应一种形式的持久化存储操作。如果将数据存储到不同的载体中就需要使用多个管道类。</p>
<p>已经定义好了三个管道类，将数据写入到三个载体中进行存储：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"><span class="keyword">from</span> redis <span class="keyword">import</span> Redis</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DuanziproPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    fp = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 重写父类的两个方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        print(<span class="string">'我是open_spider()，我只会在爬虫开始的时候执行一次！'</span>)</span><br><span class="line">        self.fp = open(<span class="string">'duanzi.txt'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        print(<span class="string">'我是close_spider()，我只会在爬虫结束的时候执行一次！'</span>)</span><br><span class="line">        self.fp.close()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 该方法是用来接收item对象。一次只能接收一个item，说明该方法会被调用多次</span></span><br><span class="line">    <span class="comment"># 参数item：就是接收到的item对象</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="comment"># print(item)    # item其实就是一个字典</span></span><br><span class="line">        self.fp.write(item[<span class="string">'title'</span>] + <span class="string">'：'</span> + item[<span class="string">'content'</span>] + <span class="string">'\n'</span>)</span><br><span class="line">        <span class="comment"># 将item存储到本文文件</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据存储到mysql中</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MysqlPipeline</span>:</span></span><br><span class="line">    conn = <span class="literal">None</span></span><br><span class="line">    cursor = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.conn = pymysql.Connect(host=<span class="string">'127.0.0.1'</span>, port=<span class="number">3306</span>, user=<span class="string">'root'</span>, password=<span class="string">'123'</span>, database=<span class="string">'duanzi'</span>)</span><br><span class="line">        print(self.conn)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        self.cursor = self.conn.cursor()</span><br><span class="line">        sql = <span class="string">'insert into duanziwang values ("%s", "%s")'</span> % (item[<span class="string">'title'</span>], item[<span class="string">'content'</span>])</span><br><span class="line">        <span class="comment"># 事务处理</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            self.cursor.execute(sql)</span><br><span class="line">            self.conn.commit()</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            print(e)</span><br><span class="line">            self.conn.rollback()</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.cursor.close()</span><br><span class="line">        self.conn.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据写入redis</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RedisPipeline</span>:</span></span><br><span class="line">    conn = <span class="literal">None</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.conn = Redis(host=<span class="string">'127.0.0.1'</span>, port=<span class="number">6379</span>)</span><br><span class="line">        print(self.conn)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="comment"># 这一步如果报错，将redis模块的版本指定成2.10.6即可。pip install -U redis==2.10.6</span></span><br><span class="line">        self.conn.lpush(<span class="string">'duanziData'</span>, item)</span><br></pre></td></tr></table></figure>

<p>运行之前，别忘了在 MySQL 中创建新的数据库，并创建存放数据的数据表：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">create database duanzi;</span><br><span class="line">use duanzi;</span><br><span class="line">create table duanziwang (title varchar(100), content varchar(1000));</span><br></pre></td></tr></table></figure>

<p>还要在设置中配置上这三个管道类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">'duanziPro.pipelines.DuanziproPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">   <span class="string">'duanziPro.pipelines.MysqlPipeline'</span>: <span class="number">301</span>,</span><br><span class="line">   <span class="string">'duanziPro.pipelines.RedisPipeline'</span>: <span class="number">302</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>如果运行过程中， redis 出错，可以尝试更换 redis 的版本为 2.10.6：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -U redis==2.10.6</span><br></pre></td></tr></table></figure>

<p>item 不会依次提交给三个管道类，爬虫文件中的 item 只会被提交给优先级最高的那一个管道类。优先级高的管道类需要在 process_item 中返回 item，这样才能把 item 传递给下一个即将被执行的管道类。</p>
<p>运行 scrapy 项目，可以发现，文件、MySQL 和 Redis 中都有了数据。</p>
<h4 id="scrapy-的手动请求发送实现全站数据爬取"><a href="#scrapy-的手动请求发送实现全站数据爬取" class="headerlink" title="scrapy 的手动请求发送实现全站数据爬取"></a>scrapy 的手动请求发送实现全站数据爬取</h4><p>有没有想过这样的问题：为什么 start_urls 列表中的 url 会被自动进行 get 请求的发送？</p>
<p>这是因为列表中的 url 其实是被 start_requests 这个父类方法实现的 get 请求发送。</p>
<p>在爬虫源文件中，可以通过改写父类（scrapy.Spider）的 start_requests 方法，自定义开始请求的方式。默认情况下，启动 scrapy 项目会依次对 start_urls 列表中的 url 发送 requests 请求。用代码表示，大致是这样的（实际实现要稍复杂些，因为要考虑一些条件）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> u <span class="keyword">in</span> self.start_urls:</span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(url=u, callback=self.parse)</span><br></pre></td></tr></table></figure>

<p>yield 后面跟随的是发起的 HTTP 请求。我们可以指定这个方法，来控制发起的是 GET 请求还是 POST 请求：</p>
<ul>
<li><p>GET 请求要使用 scrapy.Request 方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">yield</span> scrapy.Request(url,callback)</span><br></pre></td></tr></table></figure>

<p>callback 指定的是回调的解析函数，用于解析数据</p>
</li>
<li><p>POST 请求则使用 scrapy.FormRequest 方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">yield</span> scrapy.FormRequest(url,callback,formdata)</span><br></pre></td></tr></table></figure>

<p>callback 的作用同样用于解析数据</p>
<p>formdata 是一个字典，用来传递请求参数</p>
</li>
</ul>
<p>如何将 start_urls 中的 url 默认进行 post 请求的发送？</p>
<p>重写爬虫源文件中的 start_requests 方法即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> u <span class="keyword">in</span> self.start_urls:</span><br><span class="line">       <span class="keyword">yield</span> scrapy.FormRequest(url=u,callback=self.parse)</span><br></pre></td></tr></table></figure>

<p>有了这些基础，我们就可以实现手动请求爬取全站数据了。</p>
<p>需求：爬取段子王前 5 页的数据</p>
<p>网址 url：<a href="https://duanziwang.com/category/经典段子/1/" target="_blank" rel="noopener">https://duanziwang.com/category/经典段子/1/</a></p>
<p>爬虫源文件实现代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> handReqPro.items <span class="keyword">import</span> HandreqproItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DuanziSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'duanzi'</span></span><br><span class="line">    <span class="comment"># allowed_domains = ['duanziwang.com/category/经典段子']</span></span><br><span class="line">    start_urls = [<span class="string">'https://duanziwang.com/category/经典段子/1/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 通用的url模板</span></span><br><span class="line">    url = <span class="string">'https://duanziwang.com/category/经典段子/%s/'</span></span><br><span class="line">    page = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 父类方法：这个是该方法的原始实现</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> u <span class="keyword">in</span> self.start_urls:</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=u, callback=self.parse)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将段子网中所有页码对应的数据进行爬取</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment"># 数据解析标题和内容</span></span><br><span class="line">        article_list = response.xpath(<span class="string">'//main/article'</span>)</span><br><span class="line">        <span class="keyword">for</span> article <span class="keyword">in</span> article_list:</span><br><span class="line">            <span class="comment"># title和content是我们解析出来要保存的数据</span></span><br><span class="line">            title = article.xpath(<span class="string">'./div[1]/h1/a/text()'</span>).extract_first()</span><br><span class="line">            content = article.xpath(<span class="string">'./div[2]/p/text()'</span>).extract_first()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 实例化一个item类型的对象，将解析到的数据存储到该对象中</span></span><br><span class="line">            item = HandreqproItem()</span><br><span class="line">            <span class="comment"># 不可以通过.的形式调用属性</span></span><br><span class="line">            item[<span class="string">'title'</span>] = title</span><br><span class="line">            item[<span class="string">'content'</span>] = content</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 将item对象提交给管道</span></span><br><span class="line">            <span class="keyword">yield</span> item</span><br><span class="line">        <span class="keyword">if</span> self.page &lt;= <span class="number">5</span>:    <span class="comment"># 结束递归的条件</span></span><br><span class="line">            new_url = self.url % self.page    <span class="comment"># 其他页码对应的完整url</span></span><br><span class="line">            self.page += <span class="number">1</span></span><br><span class="line">            <span class="comment"># 对新的页码对应的url进行请求发送（手动请求GET发送）</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(new_url, self.parse)</span><br></pre></td></tr></table></figure>

<p>不要忘了在配置文件中，无视 robots，进行 UA 伪装，设置日志级别，配置管道 pipeline：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">LOG_LEVEL = <span class="string">'ERROR'</span></span><br><span class="line"></span><br><span class="line">USER_AGENT = <span class="string">'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36'</span></span><br><span class="line"></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">'handReqPro.pipelines.HandreqproPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>还要在 items 设置字段信息：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HandreqproItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    content = scrapy.Field()</span><br></pre></td></tr></table></figure>

<p>在管道 <code>pipelines.py</code> 中可以进行数据的存储化操作，这里仅打印出来示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HandreqproPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        print(item)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>

<p>运行 scrapy 项目，不出意外的话，即可打印出笑话网前五页的内容了。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sliu.vip/crawler/js-decode/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="刘硕">
      <meta itemprop="description" content="不成为自己讨厌的人">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="刘硕的技术查阅手册">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/crawler/js-decode/" class="post-title-link" itemprop="url">JS 解密和混淆破解</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-03-30 00:05:29 / 修改时间：00:05:32" itemprop="dateCreated datePublished" datetime="2020-03-30T00:05:29+08:00">2020-03-30</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%88%AC%E8%99%AB/" itemprop="url" rel="index"><span itemprop="name">爬虫</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/crawler/js-decode/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/crawler/js-decode/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>我们已经学到很多反爬机制以及相应的反反爬策略。是用那些手段，其实以及完全可以完成绝大多数的爬虫任务。但是，还是有极个别的情况下，会出现诸如 JS 加密和 JS 混淆之类的高深反爬机制。</p>
<p>如果不幸遇到这种反爬机制，一个明智之举是给站长点个赞，然后恭恭敬敬选择放弃，去别的地方找数据。</p>
<p>当然，还是那句话，我们可以选择不爬，但是对付 JS 加密和 JS 混淆的方法却不可以不会。</p>
<p>这里就以中国空气质量在线检测平台为例，介绍 JS 加密和 JS 混淆的实现和破解方法。</p>
<p>要爬取的网站：<a href="https://www.aqistudy.cn/html/city_detail.html" target="_blank" rel="noopener">https://www.aqistudy.cn/html/city_detail.html</a></p>
<p>这个网站正在升级，所以页面无法正常显示。这也意味着这个网站本身的 JS 解密是有问题的（如果没问题就能显示了），所以最后我们并不能完全解析出数据来。虽然如此，这个网站仍然是学习 JS 加密和 JS 混淆的相当不错的平台。</p>
<p>闲话少说，开始干活！</p>
<p>首先浏览器打开网页，并打开调试台的抓包工具。修改查询条件（城市的名称 + 时间范围），然后点击查询按钮，捕获点击按钮后发起请求对应的数据包。点击查询按钮后，并没有刷新页面，显然发起的是 ajax 请求。该请求就会将指定查询条件对应的数据加载到当前页面中（我们要爬取的数据就是该ajax请求请求到的数据）。</p>
<img src="/crawler/js-decode/1585224921475.png" class="" width="1585224921475">

<p>分析捕获到的数据包</p>
<ul>
<li>提取出请求的url：<a href="https://www.aqistudy.cn/apinew/aqistudyapi.php" target="_blank" rel="noopener">https://www.aqistudy.cn/apinew/aqistudyapi.php</a></li>
<li>请求方式：post</li>
<li>请求参数：d: 动态变化一组数据（且加密）</li>
<li>响应数据：是加密的密文数据</li>
</ul>
<p>该数据包请求到的是密文数据，为何在前台页面显示的却是原文数据呢？</p>
<p>原来，在请求请求到密文数据后，前台接受到密文数据后使用指定的解密操作（JS 函数）对密文数据进行了解密操作，然后将原文数据显示在了前台页面。</p>
<p>接下来的工作流程：</p>
<ul>
<li>首先先处理动态变化的请求参数，动态获取该参数的话，就可以携带该参数进行请求发送，将请求到的密文数据捕获到。</li>
<li>将捕获到的密文数据找到对应的解密函数对其进行解密即可。</li>
<li>【重点】需要找到点击查询按钮后对应的 ajax 请求代码，从这组代码中就可以破解动态变化的请求参数和加密的响应数据对应的相关操作。</li>
<li>找 ajax 请求对应的代码，分析代码获取参数 d 的生成方式和加密的响应数据的解密操作。<ul>
<li>直接在页面中，并没有办法直接找到发送 ajax 请求的函数的，因为它以及被封装到别的文件中了。</li>
<li>我们可以基于火狐浏览器定位查询按钮绑定的点击事件。</li>
</ul>
</li>
</ul>
<img src="/crawler/js-decode/firefoxgetactionfunction.png" class="" title="firefoxgetactionfunction">

<p>抽丝剥茧，首先从 getData 函数实现中找寻 ajax 请求对应的代码。在该函数的实现中没有找到 ajax 代码，但是发现了另外两个函数的调用，<code>getAQIData()</code> 和 <code>getWeatherData()</code>。ajax 代码一定是存在于这两个函数实现内部。</p>
<p>另外，这里记住一个参数，<code>type == ’HOUR‘</code>，它的含义是查询时间是以小时为单位。这个参数我们后来会用到。</p>
<img src="/crawler/js-decode/1585228333426.png" class="" width="1585228333426">

<p>接下来我们就去分析 <code>getAQIData()</code> 和 <code>getWeatherData()</code>，争取能够找到 ajax 代码。</p>
<p>我们找到这两个函数的定义位置，还是没有找到 ajax 请求代码。不过我们却发现它们同时调用了另外一个函数，<code>getServerData(method,param,func,0.5)</code>。它的参数的值可以为：</p>
<ul>
<li><p>method 可以是 ‘GETCITYWEATHER’ 或者 ‘GETDETAIL’</p>
</li>
<li><p>params 的值是 <code>{city, type, startTime, endTime}</code>，也就是查询条件</p>
</li>
<li><p>func 是一个匿名函数，看样子是在处理数据。</p>
</li>
</ul>
<p>下一步当然就要找 <code>getServerData</code> 函数了，看看那个函数里面有没有我们一致想要的发送 ajax 请求的代码。</p>
<p>我们尝试着在页面中搜索，却找不到这个函数。很显然，它是被封装到其他 js 文件中了。这时，我们可以基于抓包工具做全局搜索。</p>
<img src="/crawler/js-decode/1585229014619.png" class="" width="1585229014619">

<p>好消息是，我们顺利找到了 getServerData 函数！坏消息是，这货长得一点也不像是函数。</p>
<p>这是因为，这段 JS 函数代码被加密的。这种加密的方式，我们称为 JS 混淆。</p>
<p>JS 混淆，也就是对核心的 JS 代码进行加密。</p>
<p>JS 反混淆，则是对 JS 加密代码进行解密。</p>
<p>接下来我们要做的，就是 JS 反混淆，让这段我们看不懂的东西，显现出庐山真面目。</p>
<p>我们用的方法十分简单粗暴，也就是暴力破解。使用这个网站就可以实现对 JS 混淆的暴力破解：<a href="https://www.bm8.com.cn/jsConfusion/" target="_blank" rel="noopener">https://www.bm8.com.cn/jsConfusion/</a></p>
<p>将 getServerData 函数所在的那一整行代码都复制过来，粘贴到这个网址的文本输入框中，然后点击 <code>开始格式化</code> 即可：</p>
<img src="/crawler/js-decode/1585229309779.png" class="" width="1585229309779">

<p>终于，我们看到了 getServerData 的代码，并且在其中发现了发送 ajax 的请求：</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">getServerData</span>(<span class="params">method, object, callback, period</span>) </span>&#123;</span><br><span class="line">    <span class="keyword">const</span> key = hex_md5(method + <span class="built_in">JSON</span>.stringify(object));</span><br><span class="line">    <span class="keyword">const</span> data = getDataFromLocalStorage(key, period);</span><br><span class="line">    <span class="keyword">if</span> (!data) &#123;</span><br><span class="line">        <span class="keyword">var</span> param = getParam(method, object);</span><br><span class="line">        $.ajax(&#123;</span><br><span class="line">            url: <span class="string">'../apinew/aqistudyapi.php'</span>,</span><br><span class="line">            data: &#123;</span><br><span class="line">                d: param</span><br><span class="line">            &#125;,</span><br><span class="line">            type: <span class="string">"post"</span>,</span><br><span class="line">            success: <span class="function"><span class="keyword">function</span> (<span class="params">data</span>) </span>&#123;</span><br><span class="line">                data = decodeData(data);</span><br><span class="line">                obj = <span class="built_in">JSON</span>.parse(data);</span><br><span class="line">                <span class="keyword">if</span> (obj.success) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (period &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                        obj.result.time = <span class="keyword">new</span> <span class="built_in">Date</span>().getTime();</span><br><span class="line">                        localStorageUtil.save(key, obj.result)</span><br><span class="line">                    &#125;</span><br><span class="line">                    callback(obj.result)</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="built_in">console</span>.log(obj.errcode, obj.errmsg)</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        callback(data)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>从这段代码中，我们不难得出下面这几个信息：</p>
<ul>
<li>ajax 请求成功后获得到的 data 是加密的响应数据（就是我们最开始通过抓包工具看到的那一串神秘的相应字符串），通过 <code>decodeData(data)</code> 函数，可以将加密的数据解密成我们需要的明文数据。</li>
<li>发送请求时携带的参数，也就是 d 对应的值 param 是通过 <code>getParam(method, object)</code> 函数返回动的态变化的请求参数。这两个参数我们前面也分析过：<ul>
<li>参数 method 可以是 ‘GETCITYWEATHER’ 或者 ‘GETDETAIL’</li>
<li>参数 object 则为 <code>{city, type, startTime, endTime}</code>，是我们的查询条件</li>
</ul>
</li>
</ul>
<p>我们当然还可以继续最终下去，刨根问题找到它们究竟是通过什么方式进行加密和解密的。然后，使用 Python 代码，重复这个加密和解密的过程，完成请求数据的生成和响应数据的解析过程。</p>
<p>但是我们并不打算这么做。因为再继续深挖下去，难度将会陡然增加。此时我们已经很疲惫了，如果继续下去恐怕要疯掉。而且，JavaScript 和 Python 毕竟是两种语言，它们之间的方法和各种包都不相同。JavaScript 能实现的，Python 未必能够轻松完成。所以重新写一个加密和解密的脚本，并不是明智之举。</p>
<p>更好的解决方案是，我们提供请求的明文数据，通过网站自己的 JS 代码进行加密，得到加密的请求参数。使用这个参数，我们发送请求给服务端。拿到加密的响应数据后，再通过网站的 JS 代码进行解密。</p>
<p>也就是说，我们接下来需要做的就是要调用两个 JS 函数 decodeData 和 getParam，并拿到返回结果即可。</p>
<p>现在的问题是，在 Python 程序中如何调用 JS 函数呢？</p>
<p>这就涉及到一个新的概念：<strong>JS 逆向</strong>。JS 逆向，也就是在 Python 中调用 JS 函数代码。</p>
<p>能够实现 JS 逆向的方式有两种：</p>
<ol>
<li><p>手动将 JS 函数改写称为 Python 函数并执行。</p>
<p>这种方法我刚刚谈过了，并不现实。因为 JS 能实现的，Python 未必能够轻易实现。而且毕竟还要重写函数，比较麻烦。</p>
</li>
<li><p>使用固定模块，实现自动逆向（推荐）。</p>
<p>一个很好用的实现 JS 逆向的 Python 库 是 PyExecJS。</p>
<p>PyExecJS 库用来实现模拟 JavaScript 代码执行获取动态加密的请求参数，然后再将加密的响应数据带入 decodeData 进行解密即可。</p>
<p>PyExecJS 需要在本机安装好 nodejs 的环境。</p>
<p>PyExecJS 的安装：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install PyExecJS</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>接下来，我们就可以生成加密的请求数据了。</p>
<p>首先，把我们解析出来的那串代码保存到本地，比如名为 <code>code.js</code> 的文件中。在里面我们补充一个函数，比如名字叫 getPostParamCode，用来发起我们的数据请求。之所以这样做是因为使用 PyExecJS 调用 JS 函数时，传入的参数只能是字符串。而 getParam 方法的参数需要用到 JS 的自定义对象。</p>
<p>我们只需在 <code>code.js</code> 中加上下面的代码即可：</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">getPostParamCode</span>(<span class="params">method, type, city, start_time, end_time</span>) </span>&#123;</span><br><span class="line">    <span class="keyword">var</span> param = &#123;&#125;;</span><br><span class="line">    param.type = type;</span><br><span class="line">    param.city = city;</span><br><span class="line">    param.start_time = start_time;</span><br><span class="line">    param.end_time = end_time;</span><br><span class="line">    <span class="keyword">return</span> getParam(method, param)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>然后，使用 PyExecJS 调用里面的 getParam 方法，将我们的请求数据加密：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模拟执行decodeData的js函数对加密响应数据进行解密</span></span><br><span class="line"><span class="keyword">import</span> execjs</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">node = execjs.get()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 请求参数</span></span><br><span class="line">method = <span class="string">'GETCITYWEATHER'</span></span><br><span class="line">type = <span class="string">'HOUR'</span></span><br><span class="line">city = <span class="string">'北京'</span></span><br><span class="line">start_time = <span class="string">'2020-03-20 00:00:00'</span></span><br><span class="line">end_time = <span class="string">'2020-03-25 00:00:00'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 编译js代码</span></span><br><span class="line">file = <span class="string">'code.js'</span>    <span class="comment"># js代码的路径</span></span><br><span class="line">ctx = node.compile(open(file, encoding=<span class="string">'utf-8'</span>).read())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将请求数据加密</span></span><br><span class="line">encode_js = <span class="string">f'getPostParamCode("<span class="subst">&#123;method&#125;</span>", "<span class="subst">&#123;type&#125;</span>", "<span class="subst">&#123;city&#125;</span>", "<span class="subst">&#123;start_time&#125;</span>", "<span class="subst">&#123;end_time&#125;</span>")'</span></span><br><span class="line">params = ctx.eval(encode_js)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用加密的参数，发起post请求</span></span><br><span class="line">url = <span class="string">'https://www.aqistudy.cn/apinew/aqistudyapi.php'</span></span><br><span class="line">response_text = requests.post(url, data=&#123;<span class="string">'d'</span>: params&#125;).text</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将响应数据解密</span></span><br><span class="line">decode_js = <span class="string">f'decodeData("<span class="subst">&#123;response_text&#125;</span>")'</span></span><br><span class="line">decrypted_data = ctx.eval(decode_js)    <span class="comment"># 如果顺利，返回的将是解密后的原文数据</span></span><br><span class="line">print(decrypted_data)    <span class="comment"># 执行会报错：目前页面中没有数据。解密函数只是针对页面中原始的数据进行解密。</span></span><br></pre></td></tr></table></figure>

<p>自此，我们完成了 JS 加密和 JS 混淆的处理。这里我们总结一下这几个概念：</p>
<ul>
<li>JS 加密，也就是通过 JS 代码，将数据进行加密处理，将明文数据变成密文数据。如果不能将其解密，密文数据将毫无用处。</li>
<li>JS 解密：通过 JS 代码，将加密的数据解密，也就是将密文数据解析成明文数据。JS 解密是 JS 加密的逆过程。</li>
<li>JS 混淆：将 JS 代码（比如 JS 函数）本身进行加密。</li>
<li>JS 反混淆：将加密了的 JS 代码解密成常规的 JS 代码。通常直接使用暴力破解即可。</li>
<li>JS 逆向（重要）：通过 Python 代码调用 JS 的函数。</li>
</ul>
<p>附，ajax 请求的各个数据的含义：</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$(<span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>&#123; </span><br><span class="line">    $(<span class="string">".submit"</span>).on(<span class="string">"click"</span>, <span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>&#123;</span><br><span class="line">        <span class="keyword">var</span> sub = $(<span class="string">".search"</span>).val()	<span class="comment">//拿到输入框的值</span></span><br><span class="line">        $.ajax(&#123;</span><br><span class="line">            type: <span class="string">"POST"</span>,    <span class="comment">//请求类型 </span></span><br><span class="line">            url: <span class="string">'https://www.aqistudy.cn/apinew/aqistudyapi.php'</span>,    <span class="comment">//请求地址和参数    GET请求才把参数写在这里</span></span><br><span class="line">            data: d</span><br><span class="line">            <span class="comment">// res加密的响应数据</span></span><br><span class="line">            success: <span class="function"><span class="keyword">function</span> (<span class="params">res</span>) </span>&#123;   <span class="comment">//请求成功后执行的函数res是返回的值</span></span><br><span class="line">                xxxx <span class="comment">// 在对res的密文数据进行解密</span></span><br><span class="line">                                     </span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sliu.vip/crawler/mobile/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="刘硕">
      <meta itemprop="description" content="不成为自己讨厌的人">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="刘硕的技术查阅手册">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/crawler/mobile/" class="post-title-link" itemprop="url">使用 Fiddle 实现移动端数据的爬取</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-03-30 00:05:29 / 修改时间：00:05:32" itemprop="dateCreated datePublished" datetime="2020-03-30T00:05:29+08:00">2020-03-30</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%88%AC%E8%99%AB/" itemprop="url" rel="index"><span itemprop="name">爬虫</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/crawler/mobile/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/crawler/mobile/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>移动端数据的爬取，主要指的针对手机、平板电脑等便携设备的应用数据进行爬取。</p>
<p>因为移动端应用获取数据也是使用 HTTP 协议，通过发送请求获取到数据。所以我们只需要获取到移动端应用的请求接口，给足所需的参数，就可以实现移动端数据的爬取。</p>
<h3 id="Fiddle-的介绍与安装"><a href="#Fiddle-的介绍与安装" class="headerlink" title="Fiddle 的介绍与安装"></a>Fiddle 的介绍与安装</h3><p>我们刚刚说到，要爬取移动端的数据，只需要获取到应用的接口。可是人家不会主动把这些接口告诉我们，让我们去爬取他们的数据。我们就需要通过一些抓包工具，抓取这些应用发出的请求。从这些请求中，找到我们需要的那些接口。</p>
<p>对于浏览器网页而言，我们可以简单地使用调试台抓取数据。</p>
<p>在移动端，我们可以用的工具就是 Fiddle。</p>
<p>Fiddler 是位于客户端和服务器端之间的 HTTP 代理，也是目前最常用的 HTTP 抓包工具之一 。 它能够记录客户端和服务器之间的所有 HTTP 请求，可以针对特定的 HTTP 请求，分析请求数据、设置断点、调试 web 应用、修改请求的数据，甚至可以修改服务器返回的数据。可以说功能非常强大，是 web 调试的利器。</p>
<p>既然是代理，也就是说，客户端的所有请求都要先经过 Fiddler，然后转发到相应的服务器。反之，服务器端的所有响应，也都会先经过Fiddler然后发送到客户端。基于这个原因，Fiddler 支持所有可以设置 http 代理为 <code>127.0.0.1:8888</code> 的浏览器和应用程序。<br>利用可以设置代理的这个特点，我们就可以对手机 APP 进行抓包了。</p>
<p>Fiddler下载地址：<a href="https://www.telerik.com/fiddler" target="_blank" rel="noopener">https://www.telerik.com/fiddler</a></p>
<p>傻瓜式安装，一键到底。</p>
<h3 id="Fiddle-设置"><a href="#Fiddle-设置" class="headerlink" title="Fiddle 设置"></a>Fiddle 设置</h3><p>我们需要进行一些设置，让 Fiddle 能够适配我们的手机。我们需要确保手机和电脑处在同一网段之内。</p>
<p>打开 Fiddler 软件，打开工具的设置。（Fiddler 软件菜单栏：Tools-&gt;Options）</p>
<p> 在 HTTPS 中设置如下：</p>
<img src="/crawler/mobile/1585235875626.png" class="" width="1585235875626">

<p>期间可能会有系统提示，全部点击确定即可。</p>
<p>在 Connections 中设置如下，指定一个端口号，默认为 8888。端口可修改，但是注意不要与已经使用的端口冲突。</p>
<p>Allow remote computers to connect：允许别的机器把请求发送到 Fiddler 上来。</p>
<img src="/crawler/mobile/1585237248406.png" class="" width="1585237248406">

<h3 id="安全证书下载和安装"><a href="#安全证书下载和安装" class="headerlink" title="安全证书下载和安装"></a>安全证书下载和安装</h3><p>证书是需要在手机上进行安装的，这样在电脑 Fiddler 软件抓包的时候，手机使用电脑的网卡上网才不会报错。</p>
<p>在电脑浏览器中输入地址：<code>http://localhost:8888/</code>，注意修改端口号，点击 <code>FiddlerRoot certificate</code>，下载安全证书：</p>
<img src="/crawler/mobile/1585236280086.png" class="" width="1585236280086">

<h4 id="Android-手机证书安装"><a href="#Android-手机证书安装" class="headerlink" title="Android 手机证书安装"></a>Android 手机证书安装</h4><p>把证书放入手机的内置或外置存储卡上，然后通过手机的”系统安全-&gt;从存储设备安装”菜单安装证书。</p>
<p>然后找到拷贝的 <code>FiddlerRoot.cer</code> 进行安装即可。安装好之后，可以在信任的凭证中找到我们已经安装好的安全证书。</p>
<img src="/crawler/mobile/20170807155854673.png" class="" width="20170807155854673">

<h4 id="苹果手机证书安装"><a href="#苹果手机证书安装" class="headerlink" title="苹果手机证书安装"></a>苹果手机证书安装</h4><p>首先要确保手机网络和 Fiddler 所在机器网络是同一个网段下的。</p>
<p>在 Safari 中访问 <code>http://fiddle机器ip:fiddler端口</code>，进行证书下载。然后进行安装证书操作。</p>
<p>在手机中的设置 -&gt; 通用 -&gt; 关于本机 -&gt; 证书信任设置 -&gt; 开启 Fiddler 证书信任。</p>
<h3 id="局域网设置"><a href="#局域网设置" class="headerlink" title="局域网设置"></a>局域网设置</h3><p>想要使用 Fiddler 进行手机抓包，首先要确保手机和电脑的网络在一个内网中。可以使用让电脑和手机都连接同一个路由器。当然，也可以让电脑开放WIFI热点，手机连入。这里，我使用的方法是，让手机和电脑同时连入一个路由器中。最后，让手机使用电脑的代理IP进行上网。<br> 在手机上，点击连接的 WIFI 进行网络修改，添加代理。</p>
<p>进行手动设置，ip 和端口号是 fiddler 机器的 ip 和 fiddler 上设置的端口号。</p>
<h3 id="Fiddler手机抓包测试"><a href="#Fiddler手机抓包测试" class="headerlink" title="Fiddler手机抓包测试"></a>Fiddler手机抓包测试</h3><p>上述步骤都设置完成之后，用手机浏览器打开百度首页，我们就可以顺利抓包了。</p>
<p>一般情况下，移动端的数据都是通过 json 进行传输的，所以我们重点关注 json 数据即可。</p>
<p>比如我在手机上打开微博，然后就接收收到了微薄的数据：</p>
<img src="/crawler/mobile/1585237567802.png" class="" width="1585237567802">

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sliu.vip/crawler/selenium/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="刘硕">
      <meta itemprop="description" content="不成为自己讨厌的人">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="刘硕的技术查阅手册">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/crawler/selenium/" class="post-title-link" itemprop="url">selenium 模块的基本使用</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-03-30 00:05:29 / 修改时间：00:05:32" itemprop="dateCreated datePublished" datetime="2020-03-30T00:05:29+08:00">2020-03-30</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%88%AC%E8%99%AB/" itemprop="url" rel="index"><span itemprop="name">爬虫</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/crawler/selenium/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/crawler/selenium/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="selenium-介绍和安装"><a href="#selenium-介绍和安装" class="headerlink" title="selenium 介绍和安装"></a>selenium 介绍和安装</h3><p>爬虫过程中，各种反爬机制让人头疼。由于动态网页的存在，明明浏览器看得见可以点击的东西，却不能直接通过 requests 请求得到。非要绕很多弯才能获取我们想要的数据。有的请求还要携带 cookie 和一些乱七八糟的字符串。</p>
<p>selenium 模块就是为了帮我们解决这些困扰而诞生的。</p>
<p>selenium 是一种基于浏览器的自动化的模块。</p>
<p>自动化的含义是，可以通过代码指定一些列的行为动作，然后将其作用到浏览器中。</p>
<p>因为 selenium 本身就是基于浏览器的，所以几乎不会受到反爬机制的束缚。</p>
<p>selenium 的安装：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install selenium</span><br></pre></td></tr></table></figure>

<p>selenium 和爬虫之间的关联</p>
<ol>
<li>便捷的捕获到任意形式动态加载的数据（可见即可得）</li>
<li>实现模拟登录</li>
</ol>
<p>selenium 虽然能给我们带来很大的便捷，但是却有一个显著的缺点——效率太低。因为每次爬取信息，都要打开浏览器。</p>
<p>在使用 selenium 之前，我们需要下载各种浏览器的驱动，注意驱动版本要和当前使用的浏览器版本相符合。</p>
<p>谷歌驱动下载：<a href="http://chromedriver.storage.googleapis.com/index.html" target="_blank" rel="noopener">http://chromedriver.storage.googleapis.com/index.html</a></p>
<p>驱动下载好后，最好保存在环境变量中存在的文件夹中，比如 Python 安装目录的 bin 目录。这样，我们在使用 selenium 的时候，就不必每次都指定驱动了。</p>
<p>这里的示例，我把谷歌浏览器驱动放在了工作目录中，每次使用都要指定驱动的位置。</p>
<p>selenium 的基本使用流程如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> sleep</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="comment"># 基于浏览器的驱动程序实例化一个浏览器对象</span></span><br><span class="line">bro = webdriver.Chrome(executable_path=<span class="string">'./chromedriver.exe'</span>)</span><br><span class="line"><span class="comment"># 对目的网站发起请求</span></span><br><span class="line">bro.get(<span class="string">'https://www.jd.com'</span>)</span><br><span class="line"><span class="comment"># 标签定位，最常用的是id和xpath定位</span></span><br><span class="line">search_box = bro.find_element_by_xpath(<span class="string">'//input[@id="key"]'</span>)</span><br><span class="line"><span class="comment"># 向标签中输入数据</span></span><br><span class="line">search_box.send_keys(<span class="string">'iphone X'</span>)</span><br><span class="line"></span><br><span class="line">search_btn = bro.find_element_by_xpath(<span class="string">'//button[@aria-label="搜索"]'</span>)</span><br><span class="line"><span class="comment"># 点击按钮</span></span><br><span class="line">search_btn.click()</span><br><span class="line"></span><br><span class="line">sleep(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在搜索结果页面进行滚轮向下滑动的操作（执行js操作：js注入）</span></span><br><span class="line">bro.execute_script(<span class="string">'window.scrollTo(0, document.body.scrollHeight)'</span>)</span><br><span class="line">sleep(<span class="number">2</span>)</span><br><span class="line">bro.quit()</span><br></pre></td></tr></table></figure>

<p>再看一个处理百度网页的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> sleep</span><br><span class="line"></span><br><span class="line"><span class="comment"># 后面是你的浏览器驱动位置，记得前面加r'','r'是防止字符转义的</span></span><br><span class="line">driver = webdriver.Chrome(<span class="string">r'./chromedriver.exe'</span>)</span><br><span class="line"><span class="comment"># 用get打开百度页面</span></span><br><span class="line">driver.get(<span class="string">"http://www.baidu.com"</span>)</span><br><span class="line"><span class="comment"># 查找页面的“设置”选项，并进行点击</span></span><br><span class="line">driver.find_elements_by_link_text(<span class="string">'设置'</span>)[<span class="number">0</span>].click()</span><br><span class="line">sleep(<span class="number">2</span>)</span><br><span class="line"><span class="comment"># # 打开设置后找到“搜索设置”选项，设置为每页显示50条</span></span><br><span class="line">driver.find_elements_by_link_text(<span class="string">'搜索设置'</span>)[<span class="number">0</span>].click()</span><br><span class="line">sleep(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选中每页显示50条</span></span><br><span class="line">m = driver.find_element_by_id(<span class="string">'nr'</span>)</span><br><span class="line">sleep(<span class="number">2</span>)</span><br><span class="line">m.find_element_by_xpath(<span class="string">'//*[@id="nr"]/option[3]'</span>).click()</span><br><span class="line">m.find_element_by_xpath(<span class="string">'.//option[3]'</span>).click()</span><br><span class="line">sleep(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 点击保存设置</span></span><br><span class="line">driver.find_elements_by_class_name(<span class="string">"prefpanelgo"</span>)[<span class="number">0</span>].click()</span><br><span class="line">sleep(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理弹出的警告页面   确定accept() 和 取消dismiss()</span></span><br><span class="line">driver.switch_to.alert.accept()</span><br><span class="line">sleep(<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 找到百度的输入框，并输入 美女</span></span><br><span class="line">driver.find_element_by_id(<span class="string">'kw'</span>).send_keys(<span class="string">'美女'</span>)</span><br><span class="line">sleep(<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 点击搜索按钮</span></span><br><span class="line">driver.find_element_by_id(<span class="string">'su'</span>).click()</span><br><span class="line">sleep(<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 在打开的页面中找到第一个图片的标签，并打开这个页面</span></span><br><span class="line">driver.find_element_by_xpath(<span class="string">'//*[@id="1"]/div[1]/a[1]/img'</span>).click()</span><br><span class="line">sleep(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭浏览器</span></span><br><span class="line">driver.quit()</span><br></pre></td></tr></table></figure>

<p>我们从前爬取过药监局的网页，那个网页时动态生成的。当时我们是通过浏览器的抓包工具，找到获取数据的请求，从而实现了页面信息的抓取。</p>
<p>如果使用 selenium，抓取动态网页将变得很容易。</p>
<p>需求：抓取药监局页面前三页所有企业名称信息</p>
<p>网址 url：<a href="http://125.35.6.84:81/xk/" target="_blank" rel="noopener">http://125.35.6.84:81/xk/</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">'http://125.35.6.84:81/xk/'</span></span><br><span class="line">bro = webdriver.Chrome(executable_path=<span class="string">'./chromedriver'</span>)</span><br><span class="line">bro.get(url)</span><br><span class="line">page_text_list = []    <span class="comment"># 每一页的页面源码数据都会放到这里面</span></span><br><span class="line">sleep(<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 捕获到当前页面对应的页面源码数据，也就是当前页面全部加载完毕后对应的所有的数据</span></span><br><span class="line">page_text_list.append(bro.page_source)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">    next_btn = bro.find_element_by_id(<span class="string">'pageIto_next'</span>)</span><br><span class="line">    next_btn.click()</span><br><span class="line">    sleep(<span class="number">1</span>)</span><br><span class="line">    page_text_list.append(bro.page_source)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 点击下一页</span></span><br><span class="line"><span class="keyword">for</span> page_text <span class="keyword">in</span> page_text_list:</span><br><span class="line">    tree = etree.HTML(page_text)</span><br><span class="line">    text_list = tree.xpath(<span class="string">'//ul[@id="gzlist"]//dl//text()'</span>)</span><br><span class="line">    <span class="keyword">for</span> text <span class="keyword">in</span> text_list:</span><br><span class="line">        print(text)</span><br><span class="line">        </span><br><span class="line">sleep(<span class="number">2</span>)</span><br><span class="line">bro.quit()</span><br></pre></td></tr></table></figure>

<h3 id="动作链-ActionChains"><a href="#动作链-ActionChains" class="headerlink" title="动作链 ActionChains"></a>动作链 ActionChains</h3><p>有时候，我们除了点击操作之外，还会有一些比如拖动等操作。这时候，就需要用到动作链。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">bro = webdriver.Chrome(executable_path=<span class="string">'./chromedriver.exe'</span>)</span><br><span class="line">bro.get(<span class="string">'https://www.runoob.com/try/try.php?filename=jqueryui-api-droppable'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果通过find系列的函数进行标签定位，如果标签是存在于iframe下面，则会定位失败</span></span><br><span class="line"><span class="comment"># 解决方案：使用switch_to即可</span></span><br><span class="line">bro.switch_to.frame(<span class="string">'iframeResult'</span>)</span><br><span class="line">div_tag = bro.find_element_by_id(<span class="string">'draggable'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对div_tag进行滑动操作</span></span><br><span class="line">action = webdriver.ActionChains(bro)</span><br><span class="line">action.click_and_hold(div_tag)    <span class="comment"># 点击且长按</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">    <span class="comment"># perform让动作链立即执行</span></span><br><span class="line">    action.move_by_offset(<span class="number">10</span>, <span class="number">15</span>).perform()</span><br><span class="line">    sleep(<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">sleep(<span class="number">2</span>)</span><br><span class="line">bro.quit()</span><br></pre></td></tr></table></figure>

<h3 id="让-selenium-规避检测"><a href="#让-selenium-规避检测" class="headerlink" title="让 selenium 规避检测"></a>让 selenium 规避检测</h3><p>虽然 selenium 很强大，但依然有可能会漏出马脚，让人家检测出我们是在使用 selenium 发起的浏览器请求。有的网站会检测请求是否为selenium发起，如果是的话则让该次请求失败。</p>
<p>规避检测的方法是让 selenium 接管 chrome 浏览器。</p>
<p>实现步骤</p>
<ol>
<li><p>必须将你电脑中安装的谷歌浏览器的主程序所在的目录找到，且将目录添加到环境变量中。</p>
</li>
<li><p>打开 cmd，在命令行中输入命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chrome.exe --remote-debugging-port=9222 --user-data-dir=<span class="string">"一个空文件夹的目录"</span></span><br></pre></td></tr></table></figure>

<p>指定执行结束后，会打开你本机安装好的谷歌浏览器。</p>
</li>
<li><p>执行如下代码：可以使用下述代码接管步骤 2 打开的真实的浏览器</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.chrome.options <span class="keyword">import</span> Options</span><br><span class="line"> </span><br><span class="line">chrome_options = Options()</span><br><span class="line">chrome_options.add_experimental_option(<span class="string">"debuggerAddress"</span>, <span class="string">"127.0.0.1:9222"</span>)</span><br><span class="line"><span class="comment"># 本机安装好谷歌的驱动程序路径</span></span><br><span class="line">chrome_driver = <span class="string">"./chromedriver.exe"</span></span><br><span class="line"></span><br><span class="line">driver = webdriver.Chrome(executable_path=chrome_driver,chrome_options=chrome_options)</span><br><span class="line">print(driver.title)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="无头浏览器"><a href="#无头浏览器" class="headerlink" title="无头浏览器"></a>无头浏览器</h3><p>我们看到，每次执行 selenium 代码，都要打开浏览器窗口，然后亲眼看着它操作。有的时候，如果不小心把鼠标放到浏览器的窗口里面，还有可能会影响到自动化程序的正常运行。</p>
<p>无头浏览器，就是将浏览器的页面隐藏起来，我们看不见它的操作。那么就不会在每次允许 selenium 代码的时候跳出窗口来犯我们。我们也不会因为误操作而影响程序的运行。</p>
<p>常用的无头浏览器有两个：</p>
<ul>
<li>谷歌无头浏览器（推荐）</li>
<li>phantomJs（停止维护）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.chrome.options <span class="keyword">import</span> Options</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 创建一个参数对象，用来控制chrome以无界面模式打开</span></span><br><span class="line">chrome_options = Options()</span><br><span class="line">chrome_options.add_argument(<span class="string">'--headless'</span>)</span><br><span class="line">chrome_options.add_argument(<span class="string">'--disable-gpu'</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 创建浏览器对象</span></span><br><span class="line">browser = webdriver.Chrome(executable_path=<span class="string">'./chromedriver'</span>, chrome_options=chrome_options)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 上网</span></span><br><span class="line">url = <span class="string">'https://www.baidu.com/'</span></span><br><span class="line">browser.get(url)</span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br><span class="line"><span class="comment">#截图</span></span><br><span class="line">browser.save_screenshot(<span class="string">'baidu.png'</span>)</span><br><span class="line">print(browser.page_source)</span><br><span class="line">browser.quit()</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sliu.vip/crawler/get-aqistudy/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="刘硕">
      <meta itemprop="description" content="不成为自己讨厌的人">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="刘硕的技术查阅手册">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/crawler/get-aqistudy/" class="post-title-link" itemprop="url">空气质量在线监测平台首页数据爬取</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-03-30 00:05:29 / 修改时间：00:05:32" itemprop="dateCreated datePublished" datetime="2020-03-30T00:05:29+08:00">2020-03-30</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%88%AC%E8%99%AB/" itemprop="url" rel="index"><span itemprop="name">爬虫</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/crawler/get-aqistudy/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/crawler/get-aqistudy/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>在 <a href="/crawler/js-decode">JS 解密和混淆破解</a> 这篇博客中，我们尝试抓去了空气质量在线监测平台网页的数据。但是因为这个网站最近似乎在更新，前端解密不稳定，所以最终没能解析出数据来。</p>
<p>我偶然发现，这个网站的首页内容是可以正常显示的。所以，在这里尝试对其首页进行爬取。</p>
<h3 id="规避反调试机制"><a href="#规避反调试机制" class="headerlink" title="规避反调试机制"></a>规避反调试机制</h3><p>网址 url：<a href="https://www.aqistudy.cn/" target="_blank" rel="noopener">https://www.aqistudy.cn/</a></p>
<p>在这个网址的首页，如果我们使用 F12 打开调试台，网页内容会消失，就像这样：</p>
<img src="/crawler/get-aqistudy/1585239416176.png" class="" width="1585239416176">

<p>这是因为这个网站进行了反调试处理。当我们打开调试页面，会通过 JS 代码删除掉页面中的数据。</p>
<p>解决办法是，找到删除页面的 JS 代码，通过调试工具的端点功能，在其执行前，在网页中注入同名的函数将其覆盖，使得检测函数不会被执行。</p>
<p>这段 JS 代码还算比较容易找到。一个小技巧是，可以通过查找页面中显示内容所在的位置找到修改页面数据的代码。</p>
<img src="/crawler/get-aqistudy/1585240074676.png" class="" width="1585240074676">

<p>设置好断电后，刷新页面，页面会暂停在我们设置断点的位置。在 console 中，新定义一个同名的 JS 函数，覆盖掉反调试的代码，比如直接写 <code>var function endebug(){};</code> 即可：</p>
<img src="/crawler/get-aqistudy/1585240364448.png" class="" width="1585240364448">

<p>这样，我们就可以使用调试工具调试这个页面了。</p>
<h3 id="爬取指定城市的空气质量数据"><a href="#爬取指定城市的空气质量数据" class="headerlink" title="爬取指定城市的空气质量数据"></a>爬取指定城市的空气质量数据</h3><p>接下来的操作，和前一篇博客基本一致，就不复述细节了。</p>
<img src="/crawler/get-aqistudy/1585241072932.png" class="" width="1585241072932">

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  </div>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/30/">30</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="刘硕"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">刘硕</p>
  <div class="site-description" itemprop="description">不成为自己讨厌的人</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">291</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">43</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="mailto:liushuo432@outlook.com" title="E-Mail → mailto:liushuo432@outlook.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/u/2436055290" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;u&#x2F;2436055290" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="http://wpa.qq.com/msgrd?v=3&uin=1696146913&site=qq&menu=yes" title="QQ → http:&#x2F;&#x2F;wpa.qq.com&#x2F;msgrd?v&#x3D;3&amp;uin&#x3D;1696146913&amp;site&#x3D;qq&amp;menu&#x3D;yes" rel="noopener" target="_blank"><i class="fa fa-fw fa-qq"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="/atom.xml" title="RSS → &#x2F;atom.xml"><i class="fa fa-fw fa-rss"></i></a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        
  <div class="beian"><a href="http://www.beian.miit.gov.cn/" rel="noopener" target="_blank">辽ICP备20001451号 </a>
      <img src="/images/beian_icon.png" style="display: inline-block;"><a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=21142102000063" rel="noopener" target="_blank">辽公网安备 21142102000063号 </a>
  </div>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">刘硕</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script size="300" alpha="0.2" zIndex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>




  
<script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script>
<script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script>
<script src="/js/algolia-search.js"></script>














  

  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'Ga2II3wuJmHX3GiNHm9TmI97-gzGzoHsz',
      appKey     : 'esGYJQepdYLHf07E1VMsP3RK',
      placeholder: "o(*￣▽￣*)ブ来说点什么吧...（填上邮箱可以收到回复提醒）",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : true,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

  <script type="text/javascript" src="/js/love.js"></script>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"/live2dw/assets/Epsilon2.1.model.json"},"display":{"superSample":2,"width":200,"height":400,"position":"left","hOffset":-30,"vOffset":-40},"mobile":{"show":false,"scale":0.5},"react":{"opacityDefault":0.7,"opacityOnHover":0.2},"log":false,"tagMode":false});</script></body>
</html>
