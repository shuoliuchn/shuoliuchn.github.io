<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="Tu5vqPaUb8svfkPx5eetJFD84ciQCcWVXNatdsWtj9Q">
  <meta name="baidu-site-verification" content="baidu_verify_FBq9PG4BBb">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"sliu.vip","root":"/","scheme":"Mist","version":"7.7.2","exturl":false,"sidebar":{"position":"right","width":240,"display":"hide","padding":12,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":true,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":false,"nav":null,"activeClass":"valine"},"algolia":{"appID":"PXY1Z6GHKT","apiKey":"eb82f2e78e3053f26aa408e9caa96d93","indexName":"blog","hits":{"per_page":10},"labels":{"input_placeholder":"要查点什么(✿◡‿◡)","hits_empty":"没有找到任何关于 ${query} 的结果╥﹏╥...","hits_stats":"搜索到 ${hits} 条记录，用时 ${time} ms o(*￣▽￣*)ブ"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="线性回归问题的目标值是连续性的值，而分类问题的目标值是离散型的值。线性回归的作用是找出特征和目标之间存在的某种趋势。在二维平面中，该种趋势可以用一条线段来表示。">
<meta property="og:type" content="article">
<meta property="og:title" content="线性回归和回归算法的评价指标">
<meta property="og:url" content="https://sliu.vip/machine-learning/linear-regression/index.html">
<meta property="og:site_name" content="刘硕的技术查阅手册">
<meta property="og:description" content="线性回归问题的目标值是连续性的值，而分类问题的目标值是离散型的值。线性回归的作用是找出特征和目标之间存在的某种趋势。在二维平面中，该种趋势可以用一条线段来表示。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://sliu.vip/machine-learning/linear-regression/lineregressionargs.png">
<meta property="article:published_time" content="2020-04-19T16:45:40.125Z">
<meta property="article:modified_time" content="2020-04-21T16:58:12.261Z">
<meta property="article:author" content="刘硕">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="Anaconda">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="Scikit-Learn">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://sliu.vip/machine-learning/linear-regression/lineregressionargs.png">

<link rel="canonical" href="https://sliu.vip/machine-learning/linear-regression/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true
  };
</script>

  <title>线性回归和回归算法的评价指标 | 刘硕的技术查阅手册</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?f14f123935d6183fdd06f8f1c4bc378f";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="刘硕的技术查阅手册" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">刘硕的技术查阅手册</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <h1 class="site-subtitle" itemprop="description">Python 全栈开发学习笔记</h1>
      
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-toc">

    <a href="/toc/" rel="section"><i class="fa fa-fw fa-book"></i>总目录</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/404/" rel="section"><i class="fa fa-fw fa-heartbeat"></i>公益 404</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container"></div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="algolia-results">
  <div id="algolia-stats"></div>
  <div id="algolia-hits"></div>
  <div id="algolia-pagination" class="algolia-pagination"></div>
</div>

  
</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sliu.vip/machine-learning/linear-regression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="刘硕">
      <meta itemprop="description" content="不成为自己讨厌的人">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="刘硕的技术查阅手册">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          线性回归和回归算法的评价指标
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-04-20 00:45:40" itemprop="dateCreated datePublished" datetime="2020-04-20T00:45:40+08:00">2020-04-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-04-22 00:58:12" itemprop="dateModified" datetime="2020-04-22T00:58:12+08:00">2020-04-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/machine-learning/linear-regression/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/machine-learning/linear-regression/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>10k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>19 分钟</span>
            </span>
            <div class="post-description">线性回归问题的目标值是连续性的值，而分类问题的目标值是离散型的值。线性回归的作用是找出特征和目标之间存在的某种趋势。在二维平面中，该种趋势可以用一条线段来表示。</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="线性回归概述"><a href="#线性回归概述" class="headerlink" title="线性回归概述"></a>线性回归概述</h2><p>线性回归问题的目标值是连续性的值，而分类问题的目标值是离散型的值。</p>
<p>回归处理的问题为预测：</p>
<ul>
<li>预测房价</li>
<li>销售额的预测</li>
<li>设定贷款额度</li>
<li>总结：上述案例中，可以根据事物的相关特征预测出对应的结果值</li>
</ul>
<p>线性回归在生活中的映射：计算学生的期末成绩：</p>
<ul>
<li>期末成绩的制定：<code>0.7 * 考试成绩 + 0.3 平时成绩</code>，则该例子中，特征值为考试成绩和平时成绩，目标值为总成绩。从此案例中大概可以感受到<ul>
<li>回归算法预测出来的结果其实就是经过相关的算法计算出来的结果值！</li>
<li>每一个特征需要有一个权重的占比，这个权重的占比明确后，则就可以得到最终的计算结果，也就是获取到最终预测的结果了。</li>
</ul>
</li>
</ul>
<p>那么这个特征对应的权重如何获取或者如何制定呢？</p>
<p>我们来看一个预测楼房价格的例子。</p>
<p>首先导入需要的模块，并构造数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> DataFrame</span><br><span class="line">dic = &#123;</span><br><span class="line">    <span class="string">'面积'</span>: [<span class="number">55</span>, <span class="number">76</span>, <span class="number">80</span>, <span class="number">100</span>, <span class="number">120</span>, <span class="number">150</span>],</span><br><span class="line">    <span class="string">'售价'</span>: [<span class="number">110</span>, <span class="number">152</span>, <span class="number">160</span>, <span class="number">200</span>, <span class="number">240</span>, <span class="number">300</span>]</span><br><span class="line">&#125;</span><br><span class="line">df = DataFrame(dic)</span><br><span class="line">df</span><br></pre></td></tr></table></figure>

<p>构造出来的楼房价格数据为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">	售价	面积</span><br><span class="line"><span class="number">0</span>	<span class="number">110</span>	<span class="number">55</span></span><br><span class="line"><span class="number">1</span>	<span class="number">152</span>	<span class="number">76</span></span><br><span class="line"><span class="number">2</span>	<span class="number">160</span>	<span class="number">80</span></span><br><span class="line"><span class="number">3</span>	<span class="number">200</span>	<span class="number">100</span></span><br><span class="line"><span class="number">4</span>	<span class="number">240</span>	<span class="number">120</span></span><br><span class="line"><span class="number">5</span>	<span class="number">300</span>	<span class="number">150</span></span><br></pre></td></tr></table></figure>

<p>然后我们对售房数据的分布情况进行可视化展示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pylab <span class="keyword">import</span> mpl</span><br><span class="line">mpl.rcParams[<span class="string">'font.sans-serif'</span>] = [<span class="string">'FangSong'</span>]    <span class="comment"># 指定默认字体</span></span><br><span class="line">mpl.rcParams[<span class="string">'axes.unicode_minus'</span>] = <span class="literal">False</span>    <span class="comment"># 解决保存图像是负号'-'显示为方块的问题</span></span><br><span class="line"></span><br><span class="line">plt.scatter(df[<span class="string">'面积'</span>],df[<span class="string">'售价'</span>])</span><br><span class="line">plt.xlabel(<span class="string">'面积'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'售价'</span>)</span><br><span class="line">plt.title(<span class="string">'面积和价钱的分布图'</span>)</span><br></pre></td></tr></table></figure>

<p>得到房屋面积和价格关系的散点图：</p>


<p>假如现在有一套房子，面积为 76.8 平米，那么这套房子应该卖多少钱呢？也就是如何预测该套房子的价钱呢？</p>
<p>上图中散点的分布情况就是面积和价钱这两个值之间的关系，那么如果该关系可以用一个走势的直线来表示的话，那么是不是就可以通过这条走势的直线预测出新房子的价格呢？</p>
<p>为了更好地看出散点图之间的联系，我们可以绘制一条趋势线：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(df[<span class="string">'面积'</span>],df[<span class="string">'售价'</span>])</span><br><span class="line">plt.xlabel(<span class="string">'面积'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'售价'</span>)</span><br><span class="line">plt.title(<span class="string">'面积和价钱的分布图'</span>)</span><br><span class="line">plt.scatter(np.linspace(<span class="number">0</span>, <span class="number">180</span>, num=<span class="number">100</span>),np.linspace(<span class="number">0</span>, <span class="number">180</span>, num=<span class="number">100</span>) * <span class="number">2</span>, alpha=<span class="number">0.3</span>)</span><br></pre></td></tr></table></figure>

<p>得到的趋势线如图：</p>


<p>散点的趋势：</p>
<ul>
<li>在上图中使用了一条直线来表示了房子的价格和面子对应的分布趋势，那么该趋势找到后，就可以基于该趋势根据新房子的面积预测出新房子的价格。</li>
</ul>
<p>线性回归的作用：</p>
<ul>
<li>就是找出特征和目标之间存在的某种趋势。在二维平面中，该种趋势可以用一条线段来表示。</li>
</ul>
<p>该趋势使用什么表示呢？—&gt; 线性方程：</p>
<ul>
<li>在数学中，线性方程 <code>y = wx</code> 就可以表示一条唯一的直线。那么在上述售房数据中，面积和价格之间的关系（二倍的关系）其实就可以映射成<ul>
<li><code>价格 = 2 * 面积</code> ==&gt; <code>y=2x</code>，这个方程就是价格和面积的趋势！也就是说根据该方程就可以进行新房子价格的预测</li>
</ul>
</li>
<li>标准的线性方程式为：<code>y = wx + b</code>，w 为斜率，b 为截距。那么如果用线性方程表示房价和面积的趋势的话，这个 b 是否需要带上呢？</li>
</ul>


<p>是否带上 b，得具体情况具体分析。对于函数 <code>y=wx</code>，如果 x 为 0，则 y 必定为 0，那就意味着趋势对应的直线必过坐标系的原点（0，0）。如果带上 b 值，则直线不过原点。如果上有图的趋势直线过原点的话，趋势就会不准。加 b 的目的是为了使得趋势对应的直线更加具有通用性。</p>
<p>上述的线性方程 <code>y=wx+b</code> 中的 x 为特征，y 为目标，这种方程作为线性关系模型的预测依据的话是否可以满足所有的预测场景呢？</p>
<p>如果现在房价受影响的因素不光是面积了，加入了采光率和楼层了，那么就意味着特征变成了 3 种。在原始的线性方程 <code>y=wx+b</code> 中只可以有一个特征，则该方程不具备通用性。</p>
<p>标准线性关系模型为：</p>
<ul>
<li><code>面积 = (w1面积 + w2采光率 + w3楼层+ b)</code> ==&gt; <code>y = (w1x1 + w2x2 + wn * xn)+b</code><ul>
<li>w 又叫做权重。</li>
<li>b 可以变换成 <code>w0 * x0</code>，<code>x0=1</code><ul>
<li><code>y = w0 * x0 + w1 * x1 + w2 * x2 + wn * xn</code></li>
</ul>
</li>
<li>权重向量：<code>w0, w1,...wn</code></li>
<li>特征向量：<code>x0, x1,...xn</code></li>
</ul>
</li>
</ul>
<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>线性回归就是找出特征和特征权重之间的一种组合，从而来预测对应的结果。</p>
<p>线性方程式：</p>




<h2 id="矩阵基础"><a href="#矩阵基础" class="headerlink" title="矩阵基础"></a>矩阵基础</h2><p>矩阵是大多数算法的基础，非常重要</p>
<p>矩阵和数组的区别：</p>
<ul>
<li>数据可以是任意维度的</li>
<li>矩阵必须是二维的</li>
</ul>
<p>矩阵满足了一些特定的需求：</p>
<ul>
<li>矩阵乘法：（m行，b列）*（b行，n列）==（m行，n列），即左行乘右列。</li>
</ul>
<p>矩阵乘法示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">y = np.array([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]])</span><br><span class="line">display(x, y)</span><br><span class="line">np.dot(x, y)</span><br></pre></td></tr></table></figure>

<p>矩阵 x 和 y 以及它们相乘的结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">       [<span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]])</span><br><span class="line">array([[ <span class="number">5</span>,  <span class="number">5</span>,  <span class="number">5</span>],</span><br><span class="line">       [<span class="number">11</span>, <span class="number">11</span>, <span class="number">11</span>],</span><br><span class="line">       [<span class="number">17</span>, <span class="number">17</span>, <span class="number">17</span>]])</span><br></pre></td></tr></table></figure>

<p>比如我们有四个权重数据，和两组特征数据，将其进行运算可得预期值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ws = np.array([[<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]])    <span class="comment"># 每个特征的权重</span></span><br><span class="line">xs = np.array([[<span class="number">55</span>, <span class="number">66</span>, <span class="number">77</span>, <span class="number">88</span>], [<span class="number">35</span>, <span class="number">46</span>, <span class="number">97</span>, <span class="number">118</span>]])    <span class="comment">#两组特征</span></span><br><span class="line">display(xs, ws.T)</span><br><span class="line">np.dot(xs, ws.T)    <span class="comment"># 计算预期值</span></span><br></pre></td></tr></table></figure>

<p>两组矩阵和乘法结果为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">array([[ <span class="number">55</span>,  <span class="number">66</span>,  <span class="number">77</span>,  <span class="number">88</span>],</span><br><span class="line">       [ <span class="number">35</span>,  <span class="number">46</span>,  <span class="number">97</span>, <span class="number">118</span>]])</span><br><span class="line">array([[<span class="number">2</span>],</span><br><span class="line">       [<span class="number">2</span>],</span><br><span class="line">       [<span class="number">2</span>],</span><br><span class="line">       [<span class="number">2</span>]])</span><br><span class="line">array([[<span class="number">572</span>],</span><br><span class="line">       [<span class="number">592</span>]])</span><br></pre></td></tr></table></figure>

<h2 id="误差处理"><a href="#误差处理" class="headerlink" title="误差处理"></a>误差处理</h2><p>在多数的预测中都会和真实值存在一定的误差。</p>




<p>误差存在，那我们如何处理误差呢？在处理误差之前，我们必须先要知道一个回归算法的特性：</p>
<p>回归算法是一个迭代算法。所谓的迭代就好比是系统版本的迭代，迭代后的系统要比迭代前的系统更好。</p>
<ul>
<li>当开始训练线性回归模型的时候，是逐步的将样本数据带入模型对其进行训练的。</li>
<li>训练开始时先用部分的样本数据训练模型生成一组 w 和 b，对应的直线和数据对应散点的误差比较大，通过不断的带入样本数据训练模型会逐步的迭代不好（误差较大）的 w 和 b 从而使得 w 和 b 的值更加的精准。</li>
</ul>
<p>官方解释：迭代是重复反馈过程的活动，其目的通常是为了逼近所需目标或结果。每一次对过程的重复称为一次“迭代”，而每一次迭代得到的结果会作为下一次迭代的初始值。</p>
<p>通俗点来说，回归算法就是在不断的自身迭代的减少误差来使得回归算法的预测结果可以越发的逼近真实结果</p>
<p>我们可以通过 <code>损失函数</code> 来表示误差：</p>


<p>变量解释：</p>
<ul>
<li><code>yi</code>：为第 i 个训练样本的真实值</li>
<li><code>hw(xi)</code>：预测值</li>
</ul>
<p>误差的大小和我们线性回归方程中的w（权重）系数有直接的关联。那么最终的问题就转化成了，如何去求解方程中的 w 使得误差可以最小？</p>
<p>损失函数也可以表示为：</p>


<p>这个损失函数代表了向量 <code>yi-y^i</code> 的 L2 范式的平方结果。L2 范式的本质是就是欧式距离，即是两个向量上的每个点对应相减后的平方和再开平方。我们现在只实现了向量上每个点对应相减后的平方和，并没有开方，所以我们的损失函数是 L2 范式，即欧式距离的平方结果。</p>
<p>在这个平方结果下，我们的 y 和 y^ 分别是我们的真实标签和预测值，也就是说，这个损失函数实在计算我们的真实标 签和预测值之间的距离。因此，我们认为这个损失函数衡量了我们构造的模型的预测结果和真实标签的差异，因此我们固然希望我们的预测结果和真实值差异越小越好。所以我们的求解目标就可以转化为：</p>


<p>其中右下角的 2 表示向量 <code>y - Xw</code> 的 L2 范式，也就是我们的损失函数所代表的含义。在 L2 范式上求平方，就是我们的损失函数。我们往往称 呼这个式子为 SSE（Sum of Sqaured Error，误差平方和）或者 RSS（Residual Sum of Squares 残差平方和）。</p>
<h2 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h2><p>现在问题转换成了求解让 RSS 最小化的参数向量 w，这种通过最小化真实值和预测值之间的 RSS 来求解参数的方法叫做最小二乘法。</p>
<p>求解极值（最小值）的第一步往往是求解一阶导数并让一阶导数等于0，最小二乘法也不能免俗。因此，我们现在在残差平方和 RSS 上对参数向量 w 求导。</p>
<p>w 表示的是一个列向量（矩阵），我们现在并非是对常数求导，而是对列向量（矩阵）进行求导。矩阵求导的自行掌握即可。</p>
<p>首先将 L2 范式拆开，向量（<code>y - Xw</code>）的平方就等于向量的转置乘以向量本身。：</p>


<p>根据向量转置的性质，我们有：</p>


<p>拆分多项式，进行因式分解，然后根据微分的性质可得：</p>


<p>公式就变相为 yTy 对 w 求导，wTXTy 对 w 的求导，yTXw 对 w 的求导，wTXTXw 对 w 的求导。这里的 w 为列向量（矩阵）则就涉及到对矩阵的求导：</p>
<p>在矩阵求导中如果小 a 为常数项，A、B 和 C 为矩阵则有：</p>


<p>由于 y 是一个列向量，为一阶矩阵，那么其本身乘以其转置为一个常数。</p>
<p>将上述规则应用到求导结果中，有：</p>


<p>此时我们就求解出了对 w 求导的一阶导数，接下来让一阶导数为 0 则就求出了最小误差下的 w 的值。</p>


<h2 id="最小二乘法的使用"><a href="#最小二乘法的使用" class="headerlink" title="最小二乘法的使用"></a>最小二乘法的使用</h2><p>sklearn 封装了最小二乘法计算权重的算法，其 API 为：</p>
<ul>
<li>最小二乘（正规方程）：<code>from sklearn.linear_model import LinearRegression</code></li>
<li><code>coef_</code> 属性返回的就是最小误差下对应的 w</li>
</ul>
<p>常用参数：</p>
<img src="/machine-learning/linear-regression/lineregressionargs.png" class="" title="lineregressionargs">

<p>这些参数中并没有一个是必填的，更没有对我们的模型有不可替代作用的参数。这说明，线性回归的性能，往往取决于数据本身，而并非是我们的调参能力，线性回归也因此对数据有着很高的要求。幸运的是，现实中大部分连续型变量之间，都存在着或多或少的线性联系。所以线性回归虽然简单，却很强大。顺便一提，sklearn 中的线性回归可以处理多标签问题，只需要在 fit 的时候输入多维度标签就可以了。</p>
<p>normalize 参数：如果为 True，则会对特征数据进行归一化处理，如果想对特征数据进行标准化处理则需要在训练模型前调用相关工具类对其进行标准化处理。</p>
<p>接下来，我们将使用最小二乘对加利福尼亚房屋进行预测</p>
<p>特征介绍：</p>
<ul>
<li>AveBedrms：该街区平均的卧室数目</li>
<li>Population：街区人口</li>
<li>AveOccup：平均入住率</li>
<li>Latitude：街区的纬度</li>
<li>Longitude：街区的经度</li>
<li>MedInc：街区住户收入的中位数</li>
<li>HouseAge：房屋使用年数中位数</li>
<li>AveRooms：街区平均房屋的数量</li>
</ul>
<p>首先，还是导入各种需要的模块：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_california_housing <span class="keyword">as</span> fch    <span class="comment"># 加利福尼亚房屋价值数据集 </span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure>

<p>然后，拿到房屋数据的特征数据和目标数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载数据</span></span><br><span class="line">data = fch()</span><br><span class="line">feature = data.data</span><br><span class="line">target = data.target</span><br></pre></td></tr></table></figure>

<p>查看特征数据的形状：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">feature.shape    <span class="comment"># (20640, 8)</span></span><br></pre></td></tr></table></figure>

<p>查看目标数据形状：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">target.shape    <span class="comment"># (20640,)</span></span><br></pre></td></tr></table></figure>

<p>为了查看数据，我们可以将其封装到 DataFrame 中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 样本数据提取（封装到df中查看）</span></span><br><span class="line">df = pd.DataFrame(data=feature, columns=data.feature_names)</span><br><span class="line">df[<span class="string">'price'</span>] = target</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>

<p>数据前五行内容为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">	MedInc	HouseAge	AveRooms	AveBedrms	Population	AveOccup	Latitude	Longitude	price</span><br><span class="line"><span class="number">0</span>	<span class="number">8.3252</span>	<span class="number">41.0</span>	<span class="number">6.984127</span>	<span class="number">1.023810</span>	<span class="number">322.0</span>	<span class="number">2.555556</span>	<span class="number">37.88</span>	<span class="number">-122.23</span>	<span class="number">4.526</span></span><br><span class="line"><span class="number">1</span>	<span class="number">8.3014</span>	<span class="number">21.0</span>	<span class="number">6.238137</span>	<span class="number">0.971880</span>	<span class="number">2401.0</span>	<span class="number">2.109842</span>	<span class="number">37.86</span>	<span class="number">-122.22</span>	<span class="number">3.585</span></span><br><span class="line"><span class="number">2</span>	<span class="number">7.2574</span>	<span class="number">52.0</span>	<span class="number">8.288136</span>	<span class="number">1.073446</span>	<span class="number">496.0</span>	<span class="number">2.802260</span>	<span class="number">37.85</span>	<span class="number">-122.24</span>	<span class="number">3.521</span></span><br><span class="line"><span class="number">3</span>	<span class="number">5.6431</span>	<span class="number">52.0</span>	<span class="number">5.817352</span>	<span class="number">1.073059</span>	<span class="number">558.0</span>	<span class="number">2.547945</span>	<span class="number">37.85</span>	<span class="number">-122.25</span>	<span class="number">3.413</span></span><br><span class="line"><span class="number">4</span>	<span class="number">3.8462</span>	<span class="number">52.0</span>	<span class="number">6.281853</span>	<span class="number">1.081081</span>	<span class="number">565.0</span>	<span class="number">2.181467</span>	<span class="number">37.85</span>	<span class="number">-122.25</span>	<span class="number">3.422</span></span><br></pre></td></tr></table></figure>

<p>接下来，我们将特征数据和目标数据拆分成训练集和测试集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_train, x_test, y_train, y_test = train_test_split(feature, target, random_state=<span class="number">2020</span>, test_size=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>

<p>这样我们就可以使用训练集建模了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">linear = LinearRegression()</span><br><span class="line">linear.fit(x_train, y_train)    <span class="comment"># 训练模型：让模型求出误差最小对应最优的w</span></span><br></pre></td></tr></table></figure>

<p>通过 coef_ 属性，即可查看最小二乘法计算出来的回归权重：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">linear.coef_</span><br></pre></td></tr></table></figure>

<p>这些权重值为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([ <span class="number">4.38210243e-01</span>,  <span class="number">9.66229111e-03</span>, <span class="number">-1.08329143e-01</span>,  <span class="number">6.52754576e-01</span>,</span><br><span class="line">       <span class="number">-4.79939625e-06</span>, <span class="number">-3.60231961e-03</span>, <span class="number">-4.21938498e-01</span>, <span class="number">-4.34993681e-01</span>])</span><br></pre></td></tr></table></figure>

<p>数值越大，权重越高，说明该特征对结果的影响越显著，反之越不明显。</p>
<p>我们还可以通过 intercept_ 属性计算截距：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">linear.intercept_    <span class="comment"># -36.9921104465408</span></span><br></pre></td></tr></table></figure>

<p>我们计算得到的权重系数都是纯数字，看起来并不直观。我们可以通过 zip 函数，将权重和其特征名结合到一起查看：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将系数和特征名称结合在一起查看</span></span><br><span class="line">[*zip(data.feature_names, linear.coef_)]</span><br></pre></td></tr></table></figure>

<p>就得到了有特征名的系数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="string">'MedInc'</span>, <span class="number">0.4382102431938418</span>),</span><br><span class="line"> (<span class="string">'HouseAge'</span>, <span class="number">0.00966229110734124</span>),</span><br><span class="line"> (<span class="string">'AveRooms'</span>, <span class="number">-0.10832914332769084</span>),</span><br><span class="line"> (<span class="string">'AveBedrms'</span>, <span class="number">0.6527545761249733</span>),</span><br><span class="line"> (<span class="string">'Population'</span>, <span class="number">-4.799396252223811e-06</span>),</span><br><span class="line"> (<span class="string">'AveOccup'</span>, <span class="number">-0.003602319606308055</span>),</span><br><span class="line"> (<span class="string">'Latitude'</span>, <span class="number">-0.42193849756433377</span>),</span><br><span class="line"> (<span class="string">'Longitude'</span>, <span class="number">-0.43499368106442565</span>)]</span><br></pre></td></tr></table></figure>

<p>线性回归中通常是否需要对特征数据进行预处理（归一化，标准化）？</p>
<ul>
<li>先不做，如果模型的评分不好，可以尝试做一下。</li>
</ul>
<p>是否需要进行交叉验证？</p>
<ul>
<li>在 knn 中我们做交叉验证是为可画出学习曲线找出最优的超参数（k）</li>
<li>一般不需要做交叉验证，因为线性模型没有超参数</li>
</ul>
<h2 id="回归模型的评价指标"><a href="#回归模型的评价指标" class="headerlink" title="回归模型的评价指标"></a>回归模型的评价指标</h2><p>分类模型我们可以通过测试的结果是否得到正确分类来评价模型的好坏。那我们应该如何评价一个回归模型的效果呢？</p>
<p>回归类算法的模型评估一直都是回归算法中的一个难点，回归类与分类型算法的模型评估其实是相似的法则——找真实标签和预测值的差异。只不过在分类型算法中，这个差异只有一种角度来评判，那就是是否预测到了正确的分类，而在我们的回归类算法中，我们有两种不同的角度来看待回归的效果：</p>
<ol>
<li>我们是否预测到了正确的数值。</li>
<li>我们是否拟合到了足够的信息。</li>
</ol>
<p>这两种角度，分别对应着不同的模型评估指标。</p>
<h3 id="是否预测到了正确的数值"><a href="#是否预测到了正确的数值" class="headerlink" title="是否预测到了正确的数值"></a>是否预测到了正确的数值</h3><p>回忆一下 RSS 残差平方和，它的本质是预测值与真实值之间的差异，也就是从第一种角度来评估回归的效力。所以 RSS 既是我们的损失函数，也是我们回归类模型的模型评估指标之一。</p>
<p>但是，RSS 有着致命的缺点：它是一个无界的和，可以无限地大或者无限的小。我们只知道，我们想要求解最小的 RSS。从 RSS 的公式来看，它不能为负，所以 RSS 越接近 0 越好。但我们没有一个概念，究竟多小才算好，多接近 0 才算好？为了应对这种状况，sklearn 中使用 RSS  的变体，均方误差 MSE（mean squared error）来衡量我们的预测值和真实值的差异：</p>


<p>均方误差，本质是在 RSS 的基础上除以了样本总量，得到了每个样本量上的平均误差。有了平均误差，我们就可以将平均误差和我们的标签的取值范围（最大值和最小值）在一起比较，以此获得一个较为可靠的评估依据。（查看这个错误有多严重）</p>
<p>在 sklearn 当中，我们有两种方式调用这个评估指标：</p>
<ul>
<li>使用 sklearn 专用的模型评估模块 metrics 里的类 mean_squared_error</li>
<li>调用 交叉验证的类 cross_val_score 并使用里面的 scoring 参数来设置为 neg_mean_squared_error 使用均方误差</li>
</ul>
<p>使用测试集查看训练的均方误差：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模块实现的均方误差</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error <span class="keyword">as</span> mse</span><br><span class="line">mse(y_test, linear.predict(x_test))</span><br></pre></td></tr></table></figure>

<p>测试集中真实值和预测值的局方误差为：0.527302231957192</p>
<p>单单只看这个数字并不能得出训练结果是好还是坏的结论。我们可以查看测试集中的最大值和最小值，来获得一个直观的参照：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y_test.max(), y_test.min())</span><br></pre></td></tr></table></figure>

<p>测试集中真实值的最大值和最小值分别为：5.00001 和 0.325</p>
<p>我们看到，即便跟最大值比起来，0.5 的误差也不小了。如果跟 0.3 的最小值比，这个误差是不可接受的（事实上，如果使用最大值和最小值的平方进行比较会更加合适）。</p>
<p>因为均方误差是数据本身的，单独使用训练数据也可以获得均方误差的数值。如果使用特定的训练集可能会有偏差，我们可以通过交叉验证的方式获取训练集的均方误差：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cross_val_score(linear, x_train, y_train, cv=<span class="number">10</span>, scoring=<span class="string">'neg_mean_squared_error'</span>)</span><br></pre></td></tr></table></figure>

<p>我们得到交叉验证的结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([<span class="number">-0.53035714</span>, <span class="number">-0.58620156</span>, <span class="number">-0.52363407</span>, <span class="number">-0.5455884</span> , <span class="number">-0.49540431</span>,</span><br><span class="line">       <span class="number">-0.59466019</span>, <span class="number">-0.54364472</span>, <span class="number">-0.4760162</span> , <span class="number">-0.50039178</span>, <span class="number">-0.49117608</span>])</span><br></pre></td></tr></table></figure>

<p>均方误差都是负的。但是从公式来看，均方误差不可能为负。原来，sklearn 认为既然是偏差，造成的影响往往是负面的，所以把这些数据转成了负值。这并不影响我们的使用。</p>
<p>均方误差的计算公式中求得的均方误差的值不可能为负。但是 sklearn 中的参数 scoring 下，均方误差作为评判标准时，却是计算”负均方误差“（neg_mean_squared_error）。这是因为 sklearn 在计算模型评估指标的时候，会考虑指标本身的性质。均方误差本身是一种误差，所以被 sklearn 划分为模型的一种损失（loss）。在 sklearn 当中，所有的损失都使用负数表示，因此均方误差也被显示为负数了。真正的均方误差 MSE 的数值，其实就是 neg_mean_squared_error 去掉负号的数字。</p>
<p>我们可以对其取平均值，得到训练集经过交叉验证的均方误差：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cross_val_score(linear, x_train, y_train, cv=<span class="number">10</span>, scoring=<span class="string">'neg_mean_squared_error'</span>).mean()</span><br></pre></td></tr></table></figure>

<p>训练集的均方误差为：-0.5287074454857335</p>
<p>同样，我们可以将其与训练集的最大值和最小值比较：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y_train.max(), y_train.min())</span><br></pre></td></tr></table></figure>

<p>训练集目标结果的最大值和最小值分别为：5.00001 和 0.14999</p>
<p>差异同样比较大。</p>
<h3 id="绝对值误差（了解）"><a href="#绝对值误差（了解）" class="headerlink" title="绝对值误差（了解）"></a>绝对值误差（了解）</h3><p>除了 MSE，我们还有与 MSE 类似的 MAE（Mean absolute error，绝对均值误差）：</p>
<p>其表达的概念与均方误差完全一致，不过在真实标签和预测值之间的差异外我们使用的是 L1 范式（绝对值）。现实使 用中，MSE和MAE选一个来使用就好了。</p>
<ul>
<li>在 sklearn 当中，我们使用命令<ul>
<li><code>from sklearn.metrics import mean_absolute_error</code> 来调用 MAE，</li>
</ul>
</li>
<li>同时，我们也可以使用交叉验证中的<ul>
<li><code>scoring = &quot;neg_mean_absolute_error&quot;</code>， 以此在交叉验证时调用 MAE。</li>
</ul>
</li>
</ul>
<h3 id="均方根误差的使用"><a href="#均方根误差的使用" class="headerlink" title="均方根误差的使用"></a>均方根误差的使用</h3><p>方均误差的量纲是预测结果的量纲的平方，将其直接与预测结果比较并不合适。我们可以通过对均方误差开根号，得到均方根误差，也就是 RMSE（Root Mean Squared Error）。</p>
<ul>
<li>rmse = np.sqrt(metrics.mean_squared_error(y_true,y_pred))</li>
</ul>
<p>简单计算一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.sqrt(mse(linear.predict(x_test), y_test))</span><br></pre></td></tr></table></figure>

<p>均方根误差为：0.7261557904177257</p>
<p>也是蛮大的。</p>
<h2 id="是否拟合了足够的信息"><a href="#是否拟合了足够的信息" class="headerlink" title="是否拟合了足够的信息"></a>是否拟合了足够的信息</h2><p>对于回归类算法而言，只探索数据预测是否准确是不足够的。除了数据本身的数值大小之外，我们还希望我们的模型能够捕捉到数据的”规律“，比如数据的分布规律（抛物线），单调性等等。而是否捕获到这些信息是无法使用 MSE 来衡量的。</p>


<p>来看这张图，其中红色线是我们的真实标签，而蓝色线是我们的拟合模型。这是一种比较极端，但的确可能发生的情况。这张图像上，前半部分的拟合非常成功，看上去我们的真实标签和我们的预测结果几乎重合。但后半部分的拟合却非常糟糕，模型向着与真实标签完全相反的方向去了。</p>
<p>对于这样的一个拟合模型，如果我们使用 MSE 来对它进行判 断，它的 MSE 可能并不高，因为大部分样本其实都被完美拟合了，少数样本的真实值和预测值的巨大差异在被均分到每个 样本上之后，MSE 就会很小。</p>
<p>但这样的拟合结果必然不是一个好结果，因为一旦我们的新样本是处于拟合曲线的后半段的，预测结果必然会有巨大的偏差，而这不是我们希望看到的。</p>
<p>我们希望找到新的指标，除了判断预测的数值是否正确之外，还能够判断我们的模型是否拟合了足够多的数值之外的信息。</p>
<p>在我们学习降维特征选择的时候，我们提到我们使用方差来衡量数据上的信息量。如果方差越大，代表数据上的信息量越多，而这个信息量不仅包括了数值的大小，还包括了我们希望模型捕捉的那些规律。为了衡量模型对数据上的信 息量的捕捉，我们定义了 R2 来帮助我们：</p>




<p>可以使用三种方式来计算出 R2 的值：</p>
<ol>
<li>直接从 metrics 中导入 r2_score，输入预测值和真实值后打分。</li>
<li>直接从线性回归 LinearRegression 的接口 score 来进行调用。</li>
<li>在交叉验证中，输入”r2”来调用。</li>
</ol>
<p>使用 r2_score 方法对测试集打分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score</span><br><span class="line">r2_score(y_test, linear.predict(x_test))   <span class="comment"># 参数中真实值在前，预测值在后</span></span><br></pre></td></tr></table></figure>

<p>测试集的 R2 结果为：0.6097458147785049</p>
<p>我们还可以直接计算训练集的 R2：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">linear.score(x_train, y_train)</span><br></pre></td></tr></table></figure>

<p>训练集的 R2 为：0.6058104322752742</p>
<p>也可以使用交叉验证，获得更加可靠的 R2：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cross_val_score(linear, x_train, y_train, cv=<span class="number">10</span>, scoring=<span class="string">'r2'</span>).mean()</span><br></pre></td></tr></table></figure>

<p>R2 的值为：0.6012662971377246</p>
<p>我们看到，这几个 R2 的值都不很接近 1，所以线性拟合的并不好。</p>
<p>我们可以绘制曲线，更直观地观察：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">y_pred = linner.predict(x_test)</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">20</span>))</span><br><span class="line">plt.plot(range(len(y_test)),sorted(y_test),c=<span class="string">"black"</span>,label= <span class="string">"y_true"</span>)</span><br><span class="line">plt.plot(range(len(y_pred)),sorted(y_pred),c=<span class="string">"red"</span>,label = <span class="string">"y_predict"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>



<p>可见，虽然我们的大部分数据被拟合得比较好，但是图像的开头和结尾处却又着较大的拟合误差。如果我们在图像右侧分布着更多的数据，我们的模型就会越来越偏离我们真正的标签。这种结果类似于我们前面提到的，虽然在有限的数据集上将数值预测正确了，但却没有正确拟合数据的分布，如果有更多的数据进入我们的模型，那数据标签被预测错误的可能性是非常大的。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Python/" rel="tag"># Python</a>
              <a href="/tags/Anaconda/" rel="tag"># Anaconda</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
              <a href="/tags/Scikit-Learn/" rel="tag"># Scikit-Learn</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/machine-learning/ml-abstract/" rel="prev" title="机器学习概述">
      <i class="fa fa-chevron-left"></i> 机器学习概述
    </a></div>
      <div class="post-nav-item">
    <a href="/machine-learning/naive-bayes/" rel="next" title="朴素贝叶斯算法">
      朴素贝叶斯算法 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#线性回归概述"><span class="nav-number">1.</span> <span class="nav-text">线性回归概述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#线性回归"><span class="nav-number">2.</span> <span class="nav-text">线性回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#矩阵基础"><span class="nav-number">3.</span> <span class="nav-text">矩阵基础</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#误差处理"><span class="nav-number">4.</span> <span class="nav-text">误差处理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#最小二乘法"><span class="nav-number">5.</span> <span class="nav-text">最小二乘法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#最小二乘法的使用"><span class="nav-number">6.</span> <span class="nav-text">最小二乘法的使用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#回归模型的评价指标"><span class="nav-number">7.</span> <span class="nav-text">回归模型的评价指标</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#是否预测到了正确的数值"><span class="nav-number">7.1.</span> <span class="nav-text">是否预测到了正确的数值</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#绝对值误差（了解）"><span class="nav-number">7.2.</span> <span class="nav-text">绝对值误差（了解）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#均方根误差的使用"><span class="nav-number">7.3.</span> <span class="nav-text">均方根误差的使用</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#是否拟合了足够的信息"><span class="nav-number">8.</span> <span class="nav-text">是否拟合了足够的信息</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="刘硕"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">刘硕</p>
  <div class="site-description" itemprop="description">不成为自己讨厌的人</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">353</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">55</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="mailto:liushuo432@outlook.com" title="E-Mail → mailto:liushuo432@outlook.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/u/2436055290" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;u&#x2F;2436055290" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="http://wpa.qq.com/msgrd?v=3&uin=1696146913&site=qq&menu=yes" title="QQ → http:&#x2F;&#x2F;wpa.qq.com&#x2F;msgrd?v&#x3D;3&amp;uin&#x3D;1696146913&amp;site&#x3D;qq&amp;menu&#x3D;yes" rel="noopener" target="_blank"><i class="fa fa-fw fa-qq"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="/atom.xml" title="RSS → &#x2F;atom.xml"><i class="fa fa-fw fa-rss"></i></a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        
  <div class="beian"><a href="http://www.beian.miit.gov.cn/" rel="noopener" target="_blank">辽ICP备20001451号 </a>
      <img src="/images/beian_icon.png" style="display: inline-block;"><a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=21142102000063" rel="noopener" target="_blank">辽公网安备 21142102000063号 </a>
  </div>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">刘硕</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    <span title="站点总字数">2.1m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">62:43</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script size="300" alpha="0.2" zIndex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>




  
<script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script>
<script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script>
<script src="/js/algolia-search.js"></script>














  

  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'Ga2II3wuJmHX3GiNHm9TmI97-gzGzoHsz',
      appKey     : 'esGYJQepdYLHf07E1VMsP3RK',
      placeholder: "o(*￣▽￣*)ブ来说点什么吧...（填上邮箱可以收到回复提醒）",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : true,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

  <script type="text/javascript" src="/js/love.js"></script>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"/live2dw/assets/Epsilon2.1.model.json"},"display":{"superSample":2,"width":200,"height":400,"position":"left","hOffset":-30,"vOffset":-40},"mobile":{"show":false,"scale":0.5},"react":{"opacityDefault":0.7,"opacityOnHover":0.2},"log":false,"tagMode":false});</script></body>
</html>
